<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<!-- Mirrored from www.digitalhumanities.org/dhq/vol/3/1/000033/000033.html by HTTrack Website Copier/3.x [XR&CO'2010], Sat, 16 May 2015 13:07:04 GMT -->
<head><meta http-equiv="content-type" content="text/html; charset=utf-8"/><title>DHQ: Digital Humanities Quarterly: Computational Linguistics and Classical Lexicography</title><link rel="stylesheet" type="text/css" href="../../../../common/css/dhq.css"/><link rel="stylesheet" type="text/css" media="screen" href="../../../../common/css/dhq_screen.css"/><link rel="stylesheet" type="text/css" media="print" href="../../../../common/css/dhq_print.css"/><link rel="alternate" type="application/atom+xml" href="../../../../feed/news.xml"/><link rel="shortcut icon" href="../../../../common/images/favicon.ico"/><script type="text/javascript" src="../../../../common/js/javascriptLibrary.js">
                &lt;!-- Javascript functions --&gt;
            </script><script type="text/javascript">

 var _gaq = _gaq || [];
 _gaq.push(['_setAccount', 'UA-15812721-1']);
 _gaq.push(['_trackPageview']);

 (function() {
   var ga = document.createElement('script'); ga.type =
'text/javascript'; ga.async = true;
   ga.src = ('https:' == document.location.protocol ? 'https://ssl' :
'http://www') + '.google-analytics.com/ga.js';
   var s = document.getElementsByTagName('script')[0];
s.parentNode.insertBefore(ga, s);
 })();

        </script></head><body><div id="top"><div id="backgroundpic"><script type="text/javascript" src="../../../../common/js/pics.js"><!--displays banner image--></script></div><div id="banner"><div id="dhqlogo"><img src="http://www.digitalhumanities.org/dhq/common/images/dhqlogo.png" alt="DHQ Logo"/></div><div id="longdhqlogo"><img src="http://www.digitalhumanities.org/dhq/common/images/dhqlogolonger.png" alt="Digital Humanities Quarterly Logo"/></div></div><div id="topNavigation"><div id="topnavlinks"><span><a href="../../../../index.html" class="topnav">home</a></span><span><a href="../../../../submissions/index.html" class="topnav">submissions</a></span><span><a href="../../../../about/about.html" class="topnav">about dhq</a></span><span><a href="../../../../people/people.html" class="topnav">dhq people</a></span><span id="rightmost"><a href="../../../../contact/contact.html" class="topnav">contact</a></span></div><div id="search"><form action="http://www.digitalhumanities.org/dhq/findIt" method="get" onsubmit="javascript:document.location.href=cleanSearch(this.queryString.value); return false;"><div><input type="text" name="queryString" size="18"/> <input type="submit" value="Search"/></div></form></div></div></div><div id="main"><div id="leftsidebar"><div id="leftsidenav"><span>Current Issue<br/></span><ul><li><a href="../../../8/4/index.html">2014: 8.4</a></li></ul><span>Preview Issue<br/></span><ul><li><a href="../../../../preview/index.html">2015: 9.1</a></li></ul><span>Previous Issues<br/></span><ul><li><a href="../../../8/3/index.html">2014: 8.3</a></li><li><a href="../../../8/2/index.html">2014: 8.2</a></li><li><a href="../../../8/1/index.html">2014: 8.1</a></li><li><a href="../../../7/3/index.html">2013: 7.3</a></li><li><a href="../../../7/2/index.html">2013: 7.2</a></li><li><a href="../../../7/1/index.html">2013: 7.1</a></li><li><a href="../../../6/3/index.html">2012: 6.3</a></li><li><a href="../../../6/2/index.html">2012: 6.2</a></li><li><a href="../../../6/1/index.html">2012: 6.1</a></li><li><a href="../../../5/3/index.html">2011: 5.3</a></li><li><a href="../../../5/2/index.html">2011: 5.2</a></li><li><a href="../../../5/1/index.html">2011: 5.1</a></li><li><a href="../../../4/2/index.html">2010: 4.2</a></li><li><a href="../../../4/1/index.html">2010: 4.1</a></li><li><a href="../../4/index.html">2009: 3.4</a></li><li><a href="../../3/index.html">2009: 3.3</a></li><li><a href="../../2/index.html">2009: 3.2</a></li><li><a href="../index.html">2009: 3.1</a></li><li><a href="../../../2/1/index.html">2008: 2.1</a></li><li><a href="../../../1/2/index.html">2007: 1.2</a></li><li><a href="../../../1/1/index.html">2007: 1.1</a></li></ul><span>Indexes<br/></span><ul><li><a href="../../../../index/title.html"> Title</a></li><li><a href="../../../../index/author.html"> Author</a></li></ul></div><img src="http://www.digitalhumanities.org/dhq/common/images/lbarrev.png" style="margin-left : 7px;" alt="sidenavbarimg"/><div id="leftsideID"><b>ISSN 1938-4122</b><br/></div><div class="leftsidecontent"><h3>Announcements</h3><ul><li><a href="../../../../announcements/index.html#reviewers">Call for Reviewers</a></li><li><a href="../../../../announcements/index.html#submissions">Call for Submissions</a></li></ul></div><div class="leftsidecontent"><script type="text/javascript">addthis_pub  = 'dhq';</script><a href="http://www.addthis.com/bookmark.php" onmouseover="return addthis_open(this, '', '[URL]', '[TITLE]')" onmouseout="addthis_close()" onclick="return addthis_sendto()"><img src="http://s9.addthis.com/button1-addthis.gif" width="125" height="16" alt="button1-addthis.gif"/></a><script type="text/javascript" src="../../../../../../s7.addthis.com/js/152/addthis_widget.js">&lt;!-- Javascript functions --&gt;</script></div></div><div id="mainContent"><div id="printSiteTitle">DHQ: Digital Humanities Quarterly</div><div xmlns:dhqBiblio="http://digitalhumanities.org/dhq/ns/biblio" class="DHQarticle"><div id="pubInfo">Changing the Center of Gravity: Transforming Classical Studies Through
      Cyberinfrastructure<br/>2009<br/>Volume 3 Number 1</div><div class="toolbar"><form id="taporware" action="http://www.digitalhumanities.org/dhq/vol/3/1/000033/get"><div><a href="../index.html">2009 3.1</a>
                     | 
                    <a rel="external" href="000033.xml">XML</a>

| 
		   Discuss
			(<a href="000033.html#disqus_thread" data-disqus-identifier="000033">
				Comments
			</a>)
                </div></form></div><div class="DHQheader"><h1 class="articleTitle">Computational Linguistics and Classical Lexicography</h1><div class="author"><a rel="external" href="../bios.html#bamman_d">David Bamman</a> &lt;<a href="mailto:David_dot_Bamman_at_tufts_dot_edu" onclick="javascript:window.location.href='mailto:'+deobfuscate('David_dot_Bamman_at_tufts_dot_edu'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('David_dot_Bamman_at_tufts_dot_edu'); return false;">David_dot_Bamman_at_tufts_dot_edu</a>&gt;, Tufts University</div><div class="author"><a rel="external" href="../bios.html#crane_g">Gregory Crane</a> &lt;<a href="mailto:gregory_dot_crane_at_tufts_dot_edu" onclick="javascript:window.location.href='mailto:'+deobfuscate('gregory_dot_crane_at_tufts_dot_edu'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('gregory_dot_crane_at_tufts_dot_edu'); return false;">gregory_dot_crane_at_tufts_dot_edu</a>&gt;, Tufts University</div><span xmlns="" class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft.genre=article&amp;rft.atitle=Computational%20Linguistics%20and%20Classical%20Lexicography&amp;rft.jtitle=Digital%20Humanities%20Quarterly&amp;rft.stitle=DHQ&amp;rft.issn=1938-4122&amp;rft.date=2009-02-26&amp;rft.volume=003&amp;rft.issue=1&amp;rft.aulast=Bamman&amp;rft.aufirst=David&amp;rft.au=David%20Bamman&amp;rft.au=Gregory%20Crane"> </span></div><div id="DHQtext"><div id="abstract"><h2>Abstract</h2><p>Manual lexicography has produced extraordinary results for Greek and Latin, but it cannot
        in the immediate future provide for all texts the same level of coverage available for the
        most heavily studied materials. As we build a cyberinfrastructure for Classics in the
        future, we must explore the role that automatic methods can play within it. Using
        technologies inherited from the disciplines of computational linguistics and computer
        science, we can create a complement to these traditional reference works - a dynamic lexicon
        that presents statistical information about a word’s usage in context, including information
        about its sense distribution within various authors, genres and eras, and syntactic
        information as well.</p></div><div class="epigraph"><blockquote><p class="quote">...Great advances have been made in the sciences on which lexicography depends. Minute
          research in manuscript authorities has largely restored the texts of the classical
          writers, and even their orthography. Philology has traced the growth and history of
          thousands of words, and revealed meanings and shades of meaning which were long unknown.
          Syntax has been subjected to a profounder analysis. The history of ancient nations, the
          private life of the citizens, the thoughts and beliefs of their writers have been closely
          scrutinized in the light of accumulating information. Thus the student of to-day may
          justly demand of his Dictionary far more than the scholarship of thirty years ago could
          furnish. (<span class="error"><a href="#lewisshort">Advertisement for the Lewis &amp; Short Latin
        Dictionary, March 1, 1879.</a></span>)</p></blockquote></div><div id="div1" class="div div0"><div class="counter"><a href="#p1">1</a></div><div class="ptext" id="p1">The "scholarship of thirty years ago" that Lewis and Short here distance themselves from is
        Andrews' 1850 <a href="#andrews1850">Latin-English lexicon</a>, itself largely a
        translation of Freund’s German <a href="#freund1840">Wörterbuch</a> published only a
        decade before. As we design a cyberinfrastructure to support Classical Studies in the
        future, we will soon cross a similar milestone: the <a href="#old">Oxford Latin
          Dictionary</a> (1968-1982) has begun the slow process of becoming thirty years old
        (several of the earlier fascicles have already done so) and by 2012 the eclipse will be
        complete. Founded on the same lexicographic principles that produced the juggernaut
          <em class="emph">Oxford English Dictionary</em>, the <em class="emph">OLD</em> is a testament to the
        extraordinary results that rigorous manual labor can provide. It has, along with the <a href="#tll">Thesaurus Linguae Latinae</a>, provided extremely thorough coverage for
        the texts of the Golden and Silver Age in Latin literature and has driven modern scholarship
        for the past thirty years.</div><div class="counter"><a href="#p2">2</a></div><div class="ptext" id="p2">Manual methods, however, cannot in the immediate future provide for all texts the same
        level of coverage available for the most heavily studied materials, and as we think toward
        Classics in the next ten years, we must think not only of desiderata, but also of the means that would get
        us there. Like Lewis and Short, we can also say that great advances have been made over the
        past thirty years in the sciences underlying lexicography; but the "sciences" that we group
        in that statement include not only the traditional fields of paleography, philology, syntax
        and history, but computational linguistics and computer science as well.</div><div class="counter"><a href="#p3">3</a></div><div class="ptext" id="p3">Lexicographers have long used computers as an aid in dictionary production, but the recent
        rise of statistical language processing now lets us do far more: instead of using computers
        to simply expedite our largely manual labor, we can now use them to uncover knowledge that
        would otherwise lie hidden in expanses of text. Digital methods also let us deal well with
        scale. For instance, while the <em class="emph">OLD</em> focused on a canon of Classical authors that
        ends around the second century CE, Latin continued to be a productive language for the
        ensuing two millennia, with prolific writers in the Middle Ages, Renaissance and beyond. The
        Index Thomisticus [<a class="ref" href="#thomisticus">Busa 1974-1980</a>] alone contains 10.6 million words attributed
        to Thomas Aquinas and related authors, which is by itself larger than the entire corpus of
        extant classical Latin.<a class="noteRef" href="#d25407e130">[1]</a> Many handcrafted lexica exist for this period,
        from the scale of individual authors (cf. Ludwig Schütz’ 1895 <a href="#schutz1895">Thomas-Lexikon</a>) to entire periods (e.g., J. F. Niermeyer’s 1976 <a href="#niermeyer1976">Mediae Latinitatis Lexikon Minus</a>), but we can still do more:
        we can create a dynamic lexicon that can change and grow when fed with new texts, and that
        can present much more information about a word than reference works bound by the conventions
        of the printed page.</div><div class="counter"><a href="#p4">4</a></div><div class="ptext" id="p4">In deciding how we want to design a cyberinfrastructure for Classics over the next ten
        years, there is an important question that lurks between "where are we now?" and
        "where do we want to be?": where are our colleagues already? Computational
        linguistics and natural language processing generally perform best in high-resource
        languages — languages like English, on which computational research has been focusing for
        over sixty years, and for which expensive resources (such as treebanks, ontologies and
        large, curated corpora) have long been developed. Many of the tools we would want in the
        future are founded on technologies that already exist for English and other languages; our
        task in designing a cyberinfrastructure may simply be to transfer and customize them for
        Classical Studies. Classics has arguably the most well-curated collection of texts in the
        world, and the uses its scholars demand from that collection are unique. In the following I
        will document the technologies available to us in creating a new kind of reference work for
        the future — one that complements the traditional lexicography exemplified by the
        <em class="emph">OLD</em> and the <em class="emph">TLL</em> and lets scholars interact with their texts in new
        and exciting ways.</div></div><div id="div2" class="div div0"><h1 class="head">Where are we now?</h1><div class="counter"><a href="#p5">5</a></div><div class="ptext" id="p5">In answering this question, I am mainly concerned with two issues: the production of
        reference works (i.e., the act of lexicography) and the use that scholars make of them.</div><div class="counter"><a href="#p6">6</a></div><div class="ptext" id="p6">All of the reference works available in Classics are the products of manual labor, in which
        highly skilled individuals find examples of a word in context, cluster those examples into
        distinguishable "senses," and label those senses with a word or phrase in another language
        (like English) or in the source language (as with the <em class="emph">TLL</em>). In the past thirty
        years, computers have allowed this process to be significantly expedited, even in such
        simple ways as textual searching. Rather than relying on a vast network of volunteer readers
        to read through scores of books and write down "apt" sentences as they come across them (as
        with the <em class="emph">OED</em>), we can simply search our electronic corpora, find all examples
        of a word in context, and winnow through them sequentially to find those that most clearly
        illuminate the meaning of any given sense. This approach has been exploited most recently by
        the Greek Lexicon Project<a class="noteRef" href="#d25407e175">[2]</a> at the University of Cambridge, which has been
        developing a <em class="emph">New Greek Lexicon</em> since 1998 using a large database of
        electronically compiled slips (with a target completion date of 2010). Here the act of
        lexicography is still very manual, as each dictionary sense is still heavily curated, but
        the tedious job of citation collection is not.</div><div class="counter"><a href="#p7">7</a></div><div class="ptext" id="p7">We can contrast this computer-assisted lexicography with a new variety — which we might
        more properly call "computational lexicography" — that has emerged with the COBUILD project
          [<a class="ref" href="#sinclair1987">Sinclair 1987</a>] of the late 1980s. The <em class="emph">COBUILD English Language
          Dictionary</em> (1987) is a learner’s dictionary centered around a word’s use in
        context, and is created from an analysis of an evolving English textual corpus (the Bank of
        English, on which current editions of the COBUILD dictionary are based, was officially
        launched in 1991 and now includes 524 million words<a class="noteRef" href="#d25407e195">[3]</a>). This corpus
        evidence allows lexicographers to include frequency information as part of a word’s entry
        (helping learners concentrate on common words) and also to include sentences from the corpus
        that demonstrate a word’s common collocations — the words and phrases that it frequently
        appears with. By keeping the underlying corpus up to date, the editors are also able to add
        new headwords as they appear in the language, and common multi-word expressions and idioms
        (such as <em class="emph">bear fruit</em>) can also be uncovered as well.</div><div class="counter"><a href="#p8">8</a></div><div class="ptext" id="p8">This corpus-based approach has since been augmented in two dimensions. On the one hand,
        dictionaries and lexicographic resources are being built on larger and larger textual
        collections: the German <em class="emph">elexiko</em> project <span class="error"><a class="ref" href="#elexiko">#elexiko</a></span>, for instance,
        is built on a modern German corpus of 1.3 billion words, and we can expect much larger
        projects in the future as the web is exploited as a corpus.<a class="noteRef" href="#d25407e211">[4]</a> At the
        same time, researchers are also subjecting their corpora to more complex automatic processes
        to extract more knowledge from them. While word frequency and collocation analysis is
        fundamentally a task of simple counting, projects such as Kilgarriff’s Sketch Engine [<a class="ref" href="#kilgarriff2004">Kilgarriff et al. 2004</a>] also enable lexicographers to induce information about a word’s
        grammatical behavior as well.</div><div class="counter"><a href="#p9">9</a></div><div class="ptext" id="p9">In their ability to include statistical information about a word’s actual use, these
        contemporary projects are exploiting advances in computational linguistics that have been
        made over the past thirty years. Before turning, however, to how we can adapt these
        technologies in the creation of a new and complementary reference work, we must first
        address the use of such lexica.</div><div class="counter"><a href="#p10">10</a></div><div class="ptext" id="p10">Like the <em class="emph">OED</em>, Classical lexica generally include a list of citations under
        each headword, providing testimony by real authors for each sense. Of necessity, these
        citations are usually only exemplary selections, though the <em class="emph">TLL</em> provides
        comprehensive listings by Classical authors for many of its lemmata. These citations
        essentially function as an index into the textual collection. If I am interested in the
        places in Classical literature where the verb <em class="emph">libero</em> means <em class="emph">to
        acquit</em>, I can consult the <em class="emph">OLD</em> and then turn to the source texts it
        cites: Cic. <em class="emph">Ver</em>. 1.72, Plin. <em class="emph">Nat</em>. 6.90, etc. For a more
        comprehensive (but not exhaustive) comparison, I can consult the <em class="emph">TLL</em>.</div><div class="counter"><a href="#p11">11</a></div><div class="ptext" id="p11">This is what we might consider a manual form of "lemmatized searching." The Perseus Digital
        Library<a class="noteRef" href="#d25407e252">[5]</a> and the Thesaurus Linguae Graecae<a class="noteRef" href="#d25407e258">[6]</a> both
        provide a form of lemmatized searching for their respective texts, but it is a fuzzier
        variety than that presented here: a user can search for a word form such as <em class="emph">edo</em>
          (<em class="emph">to eat</em>) and simultaneously search the texts for all of its various
        inflections, but ambiguity is rampant - a lemmatized search for <em class="emph">edo</em> would also
        search for <em class="emph">est</em>, which is also an inflection of the far more common
        <em class="emph">sum</em> (<em class="emph">to be</em>). The search results are thus significantly diluted by
        a large number of false positives.</div><div class="counter"><a href="#p12">12</a></div><div class="ptext" id="p12">The advantage of the Perseus and TLG lemmatized search is that it gives scholars the
        opportunity to find all the instances of a given word form or lemma in the textual
        collections they each contain. The <em class="emph">TLL</em> may be built on a comprehensive
        collection of 10 million slips containing all of Latin literature up to 200 CE and
        selections beyond, but that complete collection can only be found housed in their archives;
        what we have in print and on CD-ROM is still only a sample. The <em class="emph">TLL</em>, however,
        is impeccable in precision, while the Perseus and TLG results are dirty. What we need is a
        resource to combine the best of both.</div></div><div id="div3" class="div div0"><h1 class="head">Where do we want to be?</h1><div class="counter"><a href="#p13">13</a></div><div class="ptext" id="p13">The <em class="emph">OLD</em> and <em class="emph">TLL</em> are not likely to become obsolete anytime soon;
        as the products of highly skilled editors and over a century of labor, the sense
        distinctions within them are highly precise and well substantiated. What we can provide in the near future, 
        however, is a complement to these resources, one that presents statistics about a
        word’s actual usage in texts — and not only in texts from the Classical period, but from any
        era for which we have electronic corpora. Heavily curated reference works provide great
        detail for a small set of texts; our complement is to provide lesser detail for
        <em class="emph">all</em> texts.</div><div class="counter"><a href="#p14">14</a></div><div class="ptext" id="p14">In order to accomplish this, we need to consider the role that automatic methods can play
        within our emerging cyberinfrastructure. I distinguish cyberinfrastructure from the vast
        corpora that exist for modern languages not only in the structure imposed upon the texts
        that comprise it, but also in the very composition of those texts: while modern reference
        corpora are typically of little interest in themselves (as mainly newswire), Classical texts
        have been the focus of scholars’ attention for millennia. The meaning of the word
          <em class="emph">child</em> in a single sentence from the <em class="emph">Wall Street Journal</em> is
        hardly a research question worth asking, except for the newspaper’s significance in being
        representative of the language at large; but this same question when asked of Vergil’s
        fourth <em class="emph">Eclogue</em> has been at the center of scholarly debate since the time of the
        emperor Constantine.<a class="noteRef" href="#d25407e316">[7]</a> We need to provide traditional scholars with the
        apparatus necessary to facilitate their own textual research. This will be true of a
        cyberinfrastructure for any historical culture, and for any future structure that develops
        for modern scholarly corpora as well.</div><div class="counter"><a href="#p15">15</a></div><div class="ptext" id="p15">We therefore must concentrate on two problems. First, how much can we automatically learn
        from a large textual collection using machine learning techniques that thrive on large
        corpora? And second, how can the vast labor already invested in handcrafted lexica help
        those techniques to learn?</div><div class="counter"><a href="#p16">16</a></div><div class="ptext" id="p16">What we can learn from such a corpus is actually quite significant. With a large bilingual
        corpus, we can induce a word sense inventory to establish a baseline for how frequently
        certain definitions of a word are manifested in actual use; we can also use the context
        surrounding each word to establish which particular definition is meant in any given
        instance. With the help of a treebank (a handcrafted collection of syntactically parsed
        sentences), we can train an automatic parser to parse the sentences in a monolingual corpus
        and extract information about a word’s subcategorization frames (the common syntactic
        arguments it appears with — for instance, that the verb <em class="emph">dono</em> (to give) requires
        a subject, direct object and indirect object), and selectional preferences (e.g., that the
        subject of the verb <em class="emph">amo</em> (to love) is typically animate). With clustering
        techniques, we can establish the semantic similarity between two words based on their
        appearance in similar contexts.</div><div class="counter"><a href="#p17">17</a></div><div class="ptext" id="p17">If we leverage all of these techniques to create a lexicon for both Latin and Greek, the
        lexical entries in each reference work could include the following:</div><div class="ptext"><ol class="list"><li class="item">a list of possible senses, weighted according to their probability;</li><li class="item">a list of instances of each sense in the source texts;</li><li class="item">a list of common subcategorization frames, weighted according to their probability;
          and</li><li class="item">a list of selectional preferences, weighted according to their probability.</li></ol></div><div class="counter"><a href="#p18">18</a></div><div class="ptext" id="p18">In creating a lexicon with these features, we are exploring two strengths of automated
        methods: they can analyze not only very large bodies of data but also provide customized
        analysis for particular texts or collections. We can thus not only identify patterns in one
        hundred and fifty million words of later Latin but also compare which senses of which words
        appear in the one hundred and fifty thousand words of Thucydides. <a href="#fig01">Figure 1</a> presents a mock-up of what a dictionary entry could look like in such a
        dynamic reference work. The first section ("Translation equivalents") presents items 1 and 2
        from the list, and is reminiscent of traditional lexica for classical languages: a list of
        possible definitions is provided along with examples of use. The main difference between a
        dynamic lexicon and those print lexica, however, lies in the scope of the examples: while
        print lexica select one or several highly illustrative examples of usage from a source text,
        we are in a position to present far more. <div id="fig01" class="figure"><a href="http://www.digitalhumanities.org/dhq/vol/3/1/000033/resources/images/libero.png" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/3/1/000033/resources/images/libero.png" alt="Mock-up of a sample entry in a dynamic lexicon."/></a><div class="caption"><div class="label">Figure 1. </div>Mock-up of a sample entry in a dynamic lexicon</div></div></div></div><div id="div4" class="div div0"><h1 class="head">How do we get there?</h1><div class="counter"><a href="#p19">19</a></div><div class="ptext" id="p19">We have already begun work on a dynamic lexicon like that shown in <a href="#fig01">Figure 1</a>[<a class="ref" href="#bamman2008">Bamman and Crane 2008</a>]. Our approach is to use already established methods in natural language
        processing; as such, our methodology involves the application of three core technologies:</div><div class="ptext"><ol class="list"><li class="item">identifying word senses from parallel texts;</li><li class="item">locating the correct sense for a word using contextual information; and</li><li class="item">parsing a text to extract important syntactic information.</li></ol></div><div class="counter"><a href="#p20">20</a></div><div class="ptext" id="p20">Each of these technologies has a long history of development both within the Perseus
        Project and in the natural language processing community at large. In the following I will
        detail how we can leverage them all to uncover large-scale usage patterns in a text.</div><div id="div4a" class="div div1"><h2 class="head">Word Sense Induction</h2><div class="counter"><a href="#p21">21</a></div><div class="ptext" id="p21">Our work on building a Latin sense inventory from a small collection of parallel texts in
          our digital library is based on that of <span class="error"><a href="#brown1991">Brown et al. 1991</a></span>
          and <span class="error"><a href="#gale1992">Gale et al. 1992</a></span>, who suggest that one way of
          objectively detecting the real senses of any given word is to analyze its translations: if
          a word is translated as two semantically distinct terms in another language, we have
            <em class="emph">prima facie</em> evidence that there is a real sense distinction. So, for
          example, the Greek word <em class="emph">archê</em> may be translated in one context as
            <em class="emph">beginning</em> and in another as <em class="emph">empire</em>, corresponding respectively
          to LSJ definitions I.1 and II.2.</div><div class="counter"><a href="#p22">22</a></div><div class="ptext" id="p22">Finding all of the translation equivalents for any given word then becomes a task of
          aligning the source text with its translations, at the level of individual words. The
          Perseus Digital Library contains at least one English translation for most of its Latin
          and Greek prose and poetry source texts. Many of these translations are encoded under the
          same canonical citation scheme as their source, but must further be aligned at the
          sentence and word level before individual word translation probabilities can be
          calculated. The workflow for this process is shown in <a href="#fig02">Figure 2.</a></div><div id="fig02" class="figure"><div class="ptext"><a href="http://www.digitalhumanities.org/dhq/vol/3/1/000033/resources/images/alignment.jpg" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/3/1/000033/resources/images/alignment.jpg" alt="Alignment workflow."/></a></div><div class="caption"><div class="label">Figure 2. </div>Alignment workflow</div></div><div class="counter"><a href="#p23">23</a></div><div class="ptext" id="p23">Since the XML files of both the source text and its translations are marked up with the
          same reference points, "chapter 1, section 1" of Tacitus' <em class="emph">Annales</em> is
          automatically aligned with its English translation (step 1). This results (for Latin at
          least) in aligned chunks of text that are 217 words long. These chunks are then aligned on
          a sentence level in step 2 using Moore’s Bilingual Sentence Aligner [<a class="ref" href="#moore2002">Moore 2002</a>], which aligns sentences that are 1-1 translations of each other
          with a very high precision (98.5% for a corpus of 10,000 English-Hindi sentence pairs [<a class="ref" href="#singh2005">Singh and Husain 2005</a>]).</div><div class="counter"><a href="#p24">24</a></div><div class="ptext" id="p24">In step 3, we then align these 1-1 sentences using GIZA++ <span class="error"><a class="ref" href="#giza">#giza</a></span>. Prior to
          alignment, all of the tokens in the source text and translation are lemmatized, where each
          word is replaced with all of the lemmas from which it can be inflected (for example, the
          Latin word <em class="emph">est</em> is replaced with <em class="emph">sum1 edo1</em> and the English word
            <em class="emph">is</em> is replaced with <em class="emph">be</em>). This word alignment is performed in
          both directions in order to discover multi-word expressions (MWEs) in the source
          language. </div><div id="fig03" class="figure"><div class="ptext"><a href="http://www.digitalhumanities.org/dhq/vol/3/1/000033/resources/images/salvus.jpg" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/3/1/000033/resources/images/salvus.jpg" alt="Sample word alignment from GIZA++."/></a></div><div class="caption"><div class="label">Figure 3. </div>Sample word alignment from GIZA++</div></div><div class="counter"><a href="#p25">25</a></div><div class="ptext" id="p25"><a href="#fig03">Figure 3</a> shows the result of this word alignment (here with
          English as the source language). The original, pre-lemmatized Latin is <em class="emph">salvum tu me
            esse cupisti</em> (Cicero, <em class="emph">Pro Plancio</em>, chapter 33). The original English
          is <em class="emph">you wished me to be safe</em>. As a result of the lemmatization process, many
          source words are mapped to multiple words in the target — most often to lemmas which share
          a common inflection. For instance, during lemmatization, the Latin word <em class="emph">esse</em>
          is replaced with the two lemmas from which it can be derived — <em class="emph">sum1</em> (<em class="emph">to
            be</em>) and <em class="emph">edo1</em> (<em class="emph">to eat</em>). If the word alignment process
          maps the source word <em class="emph">be</em> to both of these lemmas in a given sentence (as in
            <a href="#fig03">Figure 3</a>), the translation probability is divided evenly
          between them.</div><div class="counter"><a href="#p26">26</a></div><div class="ptext" id="p26">From these alignments we can calculate overall translation probabilities, which we
          currently present as an ordered list, as in <a href="#fig04">Figure 4.</a></div><div id="fig04" class="figure"><div class="ptext"><a href="http://www.digitalhumanities.org/dhq/vol/3/1/000033/resources/images/oratio.png" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/3/1/000033/resources/images/oratio.png" alt="Sense inventory for oratio induced from parallel texts."/></a></div><div class="caption"><div class="label">Figure 4. </div>Sense inventory for <em class="emph">oratio</em> induced from parallel texts</div></div><div class="counter"><a href="#p27">27</a></div><div class="ptext" id="p27">The weighted list of translation equivalents we identify using this technique can provide
          the foundation for our further lexical work. In the example above, we have induced from
          our collection of parallel texts that the headword <em class="emph">oratio</em> is primarily used
          with two senses: <em class="emph">speech</em> and <em class="emph">prayer</em>. </div><div class="counter"><a href="#p28">28</a></div><div class="ptext" id="p28">The granularity of the definitions in such a dynamic lexicon cannot approach that of
          human labor: the Lewis and Short <em class="emph">Latin Dictionary</em>, for instance, enumerates
          fourteen subsenses in varying degrees of granularity, from "speech" to "formal language"
          to the "power of oratory" and beyond. Our approach, however, does have two clear
          advantages which complement those of traditional lexica: first, this method allows us to
          include statistics about actual word usage in the corpus we derive it from. The use of
            <em class="emph">oratio</em> to signify <em class="emph">prayer</em> is not common in classical Latin, but
          since the corpus we induced this inventory from is largely composed of the
          <em class="emph">Vulgate</em> of Jerome, we are also able to mine this use of the word and include
          it in this list as well. Since the lexicon is dynamic, we can generate a sense inventory
          for an entire corpus or any part of it — so that if we were interested, for instance, in
          the use of <em class="emph">oratio</em> only until the second century CE, we can exclude the texts
          of Jerome from our analysis. And since we can run our word alignment at any time, we are
          always in a position to update the lexicon with the addition of new texts.</div><div id="fig05" class="figure"><div class="ptext"><a href="http://www.digitalhumanities.org/dhq/vol/3/1/000033/resources/images/respublica.png" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/3/1/000033/resources/images/respublica.png" alt="Sense inventory for the multi-word expression res publica induced&#xA;            from parallel texts."/></a></div><div class="caption"><div class="label">Figure 5. </div>Sense inventory for the multi-word expression <em class="emph">res publica</em> induced from
            parallel texts</div></div><div class="counter"><a href="#p29">29</a></div><div class="ptext" id="p29">Second, our word alignment also maps multi-word expressions, so we can include
          significant collocations in our lexicon as well. This allows us to provide translation
          equivalents for idioms and common phrases such as <em class="emph">res publica</em>
          (<em class="emph">republic</em>) or <em class="emph">gratias ago</em> (<em class="emph">to give thanks</em>). </div></div><div id="div4b" class="div div1"><h2 class="head">Word Sense Disambiguation</h2><div class="counter"><a href="#p30">30</a></div><div class="ptext" id="p30">Approaches to word sense disambiguation generally come in three varieties: </div><div class="ptext"><ol class="list"><li class="item">knowledge-based methods (<a href="#lesk1986">Lesk 1986</a>, <a href="#banerjee2002">Banerjee and Pedersen 2002</a>), which rely on existing
            reference works with a clear structure such as dictionaries and Wordnets [<a class="ref" href="#wordnet">Miller 1995</a>]; </li><li class="item">supervised corpus methods [<a class="ref" href="#grozea2004">Grozea 2004</a>], which train a classifier on a
            human-annotated sense corpus such as Semcor <span class="error"><a class="ref" href="#miller1993">#miller1993</a></span> or any of the
            SENSEVAL competition corpora [<a class="ref" href="#mihalcea2004">Mihalcea and Edmonds 2004</a>]; and </li><li class="item">unsupervised corpus methods, which train classifiers on "raw," unannotated text,
            either a monolingual corpus [<a class="ref" href="#mccarthy2004">McCarthy et al. 2004</a>] or parallel texts (<span class="error"><a href="#brown1991">Brown et al. 1991</a></span>, <a href="#tufis2004">Tufis et al.
              2004</a>). </li></ol></div><div class="counter"><a href="#p31">31</a></div><div class="ptext" id="p31">Corpus methods (especially supervised methods) generally perform best in the SENSEVAL
          competitions — at SENSEVAL-3, the best system achieved an accuracy of 72.9% in the English
          lexical sample task and 65.1% in the English all-words task.<a class="noteRef" href="#d25407e597">[8]</a>
          Manually annotated corpora, however, are generally cost-prohibitive to create, and this is
          especially exacerbated with sense-tagged corpora, for which the human inter-annotator
          agreement is often low.</div><div class="counter"><a href="#p32">32</a></div><div class="ptext" id="p32">Since the Perseus Digital Library contains two large monolingual corpora (the canon of
          Greek and Latin classical texts) and sizable parallel corpora as well, we have
          investigated using parallel texts for word sense disambiguation. This method uses the same
          techniques we used to create a sense inventory to disambiguate words in context. After we
          have a list of possible translation equivalents for a word, we can use the surrounding
          Latin or Greek context as an indicator for which sense is meant in texts where we have no
          corresponding translation. There are several techniques available for deciding which sense
          is most appropriate given the context, and several different measures for what definition
          of "context" is most appropriate itself. One technique that we have experimented with is a
          naive Bayesian classifier (following <span class="error"><a href="#gale1992">Gale et al. 1992</a></span>), with
          context defined as a sentence-level bag of words (all of the words in the sentence
          containing the word to be disambiguated contribute equally to its disambiguation). </div><div class="counter"><a href="#p33">33</a></div><div class="ptext" id="p33">Bayesian classification is most commonly found in spam filtering. A filtering program can
          decide whether or not any given email message is spam by looking at the words that
          comprise it and comparing it to other messages that are already known to be spam — some
          words generally only appear in spam messages (e.g., <em class="emph">viagra</em>,
          <em class="emph">refinance</em>, <em class="emph">opt-out</em>, <em class="emph">shocking</em>), while others only
          appear in non-spam messages (<em class="emph">archê</em>, <em class="emph">subcategorization</em>), and some
          appear equally in both (<em class="emph">and</em>, <em class="emph">your</em>). By counting each word and
          the class (spam/not spam) it appears in, we can assign it a probability that it falls into
          one class or the other.</div><div class="counter"><a href="#p34">34</a></div><div class="ptext" id="p34">We can also use this principle to disambiguate word senses by building a classifier for
          every sense and training it on sentences where we do know the correct sense for a word.
          Just as a spam filter is trained by a user explicitly labeling a message as spam, this
          classifier can be trained simply by the presence of an aligned translation. </div><div class="counter"><a href="#p35">35</a></div><div class="ptext" id="p35">For instance, the Latin word <em class="emph">spiritus</em> has several senses, including
            <em class="emph">spirit</em> and <em class="emph">wind</em>. In our texts, when <em class="emph">spiritus</em> is
          translated as <em class="emph">wind</em>, it is accompanied by words like <em class="emph">mons</em>
          (mountain), <em class="emph">ala</em> (wing) or <em class="emph">ventus</em> (wind). When it is translated
          as <em class="emph">spirit</em>, its context has (more naturally) a religious tone, including words
          such as <em class="emph">sanctus</em> (holy) and <em class="emph">omnipotens</em> (all-powerful). If we are
          confronted with an instance of <em class="emph">spiritus</em> in a sentence for which we have no
          translation, we can disambiguate it as either <em class="emph">spirit</em> or <em class="emph">wind</em> by
          looking at its context in the original Latin.</div><div class="table"><table class="table"><tr class="row"><td valign="top" class="cell label">Latin context word</td><td valign="top" class="cell label">English translation</td><td valign="top" class="cell label">Probability of accompanying <em class="emph">spiritus</em> =
            <em class="emph">wind</em></td></tr><tr class="row"><td valign="top" class="cell">Mons</td><td valign="top" class="cell">Mountain</td><td valign="top" class="cell">98.3%</td></tr><tr class="row"><td valign="top" class="cell">Commotio</td><td valign="top" class="cell">Commotion</td><td valign="top" class="cell">98.3%</td></tr><tr class="row"><td valign="top" class="cell">Ventus</td><td valign="top" class="cell">Wind</td><td valign="top" class="cell">95.2%</td></tr><tr class="row"><td valign="top" class="cell">Ala</td><td valign="top" class="cell">Wing</td><td valign="top" class="cell">95.2%</td></tr></table><div class="caption"><div class="label">Table 1. </div>
            Latin contextual probabilities where <em class="emph">spiritus</em> = <em class="emph">wind</em>.
          </div></div><div class="table"><table class="table"><tr class="row"><td valign="top" class="cell label">Latin context word</td><td valign="top" class="cell label">English translation</td><td valign="top" class="cell label">Probability of accompanying <em class="emph">spiritus</em> =
              <em class="emph">spirit</em></td></tr><tr class="row"><td valign="top" class="cell">Sanctus</td><td valign="top" class="cell">Holy</td><td valign="top" class="cell">99.9%</td></tr><tr class="row"><td valign="top" class="cell">Testis</td><td valign="top" class="cell">Witness</td><td valign="top" class="cell">99.9%</td></tr><tr class="row"><td valign="top" class="cell">Vivifico</td><td valign="top" class="cell">Make alive</td><td valign="top" class="cell">99.9%</td></tr><tr class="row"><td valign="top" class="cell">Omnipotens</td><td valign="top" class="cell">All-powerful</td><td valign="top" class="cell">99.9%</td></tr></table><div class="caption"><div class="label">Table 2. </div>
            
              Latin contextual probabilities where <em class="emph">spiritus</em> = <em class="emph">spirit</em>.
            
          </div></div><div class="counter"><a href="#p36">36</a></div><div class="ptext" id="p36">Word sense disambiguation will be most helpful for the construction of a lexicon when we
          are attempting to determine the sense for words in context for the large body of later
          Latin literature for which there exists no English translation. By training a classifier
          on texts for which we do have translations, we will be able to determine the sense in
          texts for which we don’t: if the context of <em class="emph">spiritus</em> in a late Latin text
          includes words such as <em class="emph">mons</em> and <em class="emph">ala</em>, we can use the
          probabilities we induced from parallel texts to know with some degree of certainty that it
          refers to <em class="emph">wind</em> rather than <em class="emph">spirit</em>. This will enable us to
          include these later texts in our statistics on a word’s usage, and link these passages to
          the definition as well.</div></div><div id="div4c" class="div div1"><h2 class="head">Parsing</h2><div class="counter"><a href="#p37">37</a></div><div class="ptext" id="p37">Two of the features we would like to incorporate into a dynamic lexicon are based on a
          word’s role in syntax: subcategorization and selectional preference. A verb’s
          subcategorization frame is the set of possible combinations of surface syntactic arguments
          it can appear with. In linear, unlabeled phrase structure grammars, these frames take the
          form of, for example, <em class="emph">NP PP</em> (requiring a direct object + prepositional
          phrase, as in <em class="emph">I gave a book to John</em>) or <em class="emph">NP NP</em> (requiring two
          objects, as in <em class="emph">I gave John a book</em>). In a labeled dependency grammar, we can
          express a verb’s subcategorization as a combination of syntactic roles (e.g., OBJ OBJ).</div><div class="counter"><a href="#p38">38</a></div><div class="ptext" id="p38">A predicate’s selectional preference specifies the type of argument it generally appears
          with. The verb <em class="emph">to eat</em>, for example, typically requires its object to be a
          thing that can be eaten and its subject to have animacy, unless used metaphorically.
          Selectional preference, however, can also be much more detailed, reflecting not only a
          word class (such as <em class="emph">animate</em> or <em class="emph">human</em>), but also individual words
          themselves. For instance, the kind of arguments used with the Latin verb
          <em class="emph">libero</em> (to free) are very different in Cicero and Jerome: Cicero, as an
          orator of the republic, commonly uses it to speak of liberation from
          <em class="emph">periculum</em> (danger), <em class="emph">metus</em> (fear), <em class="emph">cura</em> (care) and
            <em class="emph">aes alienum</em> (debt); Jerome, on the other hand, uses it to speak of
          liberation from a very different set of things, such as <em class="emph">manus Aegyptorum</em> (the
          hand of the Egyptians), <em class="emph">os leonis</em> (the mouth of the lion), and
          <em class="emph">mors</em> (death).<a class="noteRef" href="#d25407e852">[9]</a> These are syntactic qualities since each
          of these arguments bears a direct syntactic relation to their head as much as they hold a
          semantic place within the underlying argument structure.</div><div class="counter"><a href="#p39">39</a></div><div class="ptext" id="p39">In order to extract this kind of subcategorization and selectional information from
          unstructured text, we first need to impose syntactic order on it. One option for imposing
          this kind of order is through manual annotation, but this option is not feasible here due
          to the sheer volume of data involved — even the more resourceful of such endeavors (such
          as the Penn Treebank <span class="error"><a class="ref" href="#penn">#penn</a></span> or the Prague Dependency Treebank <span class="error"><a class="ref" href="#pdt">#pdt</a></span>) take years to complete.</div><div class="counter"><a href="#p40">40</a></div><div class="ptext" id="p40">A second, more practical option is to assign syntactic structure to a sentence using
          automatic methods. Great progress has been made in recent years in the area of syntactic
          parsing, both for phrase structure grammars (<a href="#charniak2000">Charniak
          2000</a>, <a href="#collins1999">Collins 1999</a>) and dependency grammars (<a href="#nivre2006">Nivre et al. 2006</a>, <a href="#mcdonald2005">McDonald et al.
            2005</a>), with labeled dependency parsing achieving an accuracy rate approaching 90%
          for English (a high resource, fixed word order language) and 80% for Czech (a relatively
          free word order language like Latin and Greek). Automatic parsing generally requires the
          presence of a treebank — a large collection of manually annotated sentences — and a
          treebank’s size directly correlates with parsing accuracy: the larger the treebank, the
          better the automatic analysis.</div><div class="counter"><a href="#p41">41</a></div><div class="ptext" id="p41">We are currently in the process of creating a treebank for Latin, and have just begun work on a one-million-word treebank of Ancient Greek. Now in version 1.5, the
          Latin Dependency Treebank<a class="noteRef" href="#d25407e879">[10]</a> is composed of excerpts from eight texts, including Caesar, Cicero, Jerome, Ovid, Petronius, Propertius, Sallust and Vergil. Each
          sentence in the treebank has been manually annotated so that every word is assigned a
          syntactic relation, along with the lemma from which it is inflected and its morphological
          code (a composite of nine different morphological features: part of speech, person,
          number, tense, mood, voice, gender, case and degree). Based predominantly on the
          guidelines used for the Prague Dependency Treebank, our annotation style is also
          influenced by the Latin grammar of <a href="#pinkster1990">Pinkster (1990)</a>, and
          is founded on the principles of dependency grammar [<a class="ref" href="#melcuk1988">Mel’čuk 1988</a>]. Dependency
          grammars differ from phrase-structure grammars in that they forego non-terminal phrasal
          categories and link words themselves to their immediate heads. This is an especially
          appropriate manner of representation for languages with a free word order (such as Latin
          and Czech), where the linear order of constituents is broken up with elements of other
          constituents. A dependency grammar representation, for example, of <em class="emph">ista meam norit
             gloria canitiem</em>Propertius I.8.46 — "that glory would know my old age" — would
          look like the following:</div><div id="fig06" class="figure"><div class="ptext"><a href="http://www.digitalhumanities.org/dhq/vol/3/1/000033/resources/images/ista.png" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/3/1/000033/resources/images/ista.png" alt="Dependency grammar representation of &#xA;            ()."/></a></div><div class="caption"><div class="label">Figure 6. </div>Dependency grammar representation of <span class="foreign i">ista meam norit gloria canitiem</span>
            ("that glory would know my old age")</div></div><div class="counter"><a href="#p42">42</a></div><div class="ptext" id="p42">While this treebank is still in its infancy, we can still use it to 
train a parser to parse the volumes of unstructured Latin in our collection.  Our treebank is still too small to achieve state-of-the-art results in parsing but we can still induce valuable lexical information from its output by using a large corpus and simple 
hypothesis testing techniques to outweigh the noise of the occasional error [<a class="ref" href="#bamman2008">Bamman and Crane 2008</a>]. The key to improving this parsing accuracy is to increase the size of the annotated treebank: the better the parser, the more accurate the syntactic information we can extract from our corpus.</div></div></div><div id="div5" class="div div0"><h1 class="head">Beyond the lexicon</h1><div class="counter"><a href="#p43">43</a></div><div class="ptext" id="p43">These technologies, borrowed from computational linguistics, will give us the grounding to
        create a new kind of lexicon, one that presents information about a word’s actual usage.
        This lexicon resembles its more traditional print counterparts in that it is a work designed
        to be browsed: one looks up an individual headword and then reads its lexical entry. The
        technologies that will build this reference work, however, do so by processing a large Greek
        and Latin textual corpus. The results of this automatic processing go far beyond the
        construction of a single lexicon. </div><div class="counter"><a href="#p44">44</a></div><div class="ptext" id="p44">I noted earlier that all scholarly dictionaries include a list of citations illustrating a
        word’s exemplary use. As <a href="#fig01">Figure 1</a> shows, each entry in this new,
        dynamic lexicon ultimately ends with a list of canonical citations to fixed passages in the
        text. These citations are again a natural index to a corpus, but since they are based in an
        electronic medium, they provide the foundation for truly advanced methods of textual
        searching — going beyond a search for individual word form (as in typical search engines) to
        word sense. </div><div id="div5a" class="div div1"><h2 class="head">Searching by word sense</h2><div id="fig07" class="figure"><div class="ptext"><a href="http://www.digitalhumanities.org/dhq/vol/3/1/000033/resources/images/slave-search.png" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/3/1/000033/resources/images/slave-search.png" alt="Mock-up of a service to search Latin texts by English word sense."/></a></div><div class="caption"><div class="label">Figure 7. </div>Mock-up of a service to search Latin texts by English word sense</div></div><div class="counter"><a href="#p45">45</a></div><div class="ptext" id="p45">The ability to search a Latin or Greek text by an English translation equivalent is a
          close approximation to real cross-language information retrieval. Consider scholars
          researching Roman slavery: they could compare all passages where any number of Latin
          "slave" words appear, but this would lead to separate searches for <em class="emph">servus, serva,
            ancilla, famulus, famula, minister, ministra, puer, puella</em> etc. (and all of their
          inflections), plus many other less-common words. By searching for word sense, however, a
          scholar can simply search for <em class="emph">slave</em> and automatically be presented with all
          of the passages for which this translation equivalent applies. <a href="#fig07">Figure
            7</a> presents a mock-up of what such a service could look like.</div><div class="counter"><a href="#p46">46</a></div><div class="ptext" id="p46">Searching by word sense also allows us to investigate problems of changing orthography —
          both across authors and time: as Latin passes through the Middle Ages, for instance, the
          spelling of words changes dramatically even while meaning remains the same. So, for
          example, the diphthong <em class="emph">ae</em> is often reduced to <em class="emph">e</em>, and prevocalic
            <em class="emph">ti</em> is changed to <em class="emph">ci</em>. Even within a given time frame, spelling
          can vary, especially from poetry to prose. By allowing users to search for a sense rather
          than a specific word form, we can return all passages containing <em class="emph">saeculum, saeclum,
            seculum</em> and <em class="emph">seclum</em> — all valid forms for <em class="emph">era</em>.
          Additionally, we can automate this process to discover common words with multiple
          orthographic variations, and include these in our dynamic lexicon as well.</div></div><div id="div5c" class="div div1"><h2 class="head">Searching by selectional preference</h2><div class="counter"><a href="#p47">47</a></div><div class="ptext" id="p47">The ability to search by a predicate’s selectional preference is also a step toward
          semantic searching — the ability to search a text based on what it "means." In building
          the lexicon, we automatically assign an argument structure to all of the verbs. Once this
          structure is in place, it can stay attached to our texts and thereby be searchable in the
          future, allowing us to search a text for the subjects and direct objects of any verb. Our
          scholar researching Roman slavery can use this information to search not only for passages
          where any slave has been freed (i.e., when any Latin variant of the English translation
            <em class="emph">slave</em> is the direct object of the active form of the verb
          <em class="emph">libero</em>), but also who was doing the freeing (who in such instances is the
          subject of that verb). This is a powerful resource that can give us much more information
          about a text than simple search engines currently allow.</div></div></div><div id="div6" class="div div0"><h1 class="head">Conclusion</h1><div class="counter"><a href="#p48">48</a></div><div class="ptext" id="p48">Manual lexicography has produced fantastic results for Classical languages, but as we
        design a cyberinfrastructure for Classics in the future, our aim must be to build a
        scaffolding that is essentially enabling: it must not only make historical languages more
        accessible on a functional level, but intellectually as well; it must give students the
        resources they need to understand a text while also providing scholars the tools to interact
        with it in whatever ways they see fit. In this a dynamic lexicon fills a gap left by
        traditional reference works. By creating a lexicon directly from a corpus of texts and then
        situating it within that corpus itself, we can let the two interact in ways that traditional
        lexica cannot. </div><div class="counter"><a href="#p49">49</a></div><div class="ptext" id="p49">Even driven by the scholarship of the past thirty years, however, a dynamic lexicon cannot
        yet compete with the fine sense distinctions that traditional dictionaries make, and in this
        the two works are complementary. Classics, however, is only one field among many concerned
        with the technologies underlying lexicography, and by relying on the techniques of other
        disciplines like computational linguistics and computer science, we can count on the future
        progress of disciplines far outside our own. </div></div></div><div id="notes"><h2>Notes</h2><div class="endnote" id="d25407e130"><span class="noteRef">[1]</span>The Biblioteca Teubneriana BTL-1 collection, for instance, contains 6.6 million
          words, covering Latin literature up to the second century CE. For a recent overview of the
          Index Thomisticus, including the corpus size and composition, see <a href="#busa2004">Busa
            (2004)</a>.</div><div class="endnote" id="d25407e175"><span class="noteRef">[2]</span>See <a class="ref" href="http://people.pwf.cam.ac.uk/blf10/glp/greek_lexicon_project.htm" onclick="window.open('http://people.pwf.cam.ac.uk/blf10/glp/greek_lexicon_project.htm'); return false">http://people.pwf.cam.ac.uk/blf10/GLP/Greek_Lexicon_Project.htm</a>.
        </div><div class="endnote" id="d25407e195"><span class="noteRef">[3]</span>See <a class="ref" href="http://www.collins.co.uk/books.aspx?group=153" onclick="window.open('http://www.collins.co.uk/books.aspx?group=153'); return false">http://www.collins.co.uk/books.aspx?group=153</a>.
        </div><div class="endnote" id="d25407e211"><span class="noteRef">[4]</span>In 2006, for example, Google released the first version of its Web 1T 5-gram
          corpus [<a class="ref" href="#brants2006">Brants and Franz 2006</a>] — a collection of n-grams (n=1-5) and their frequencies
          calculated from 1 trillion words of text on the web.</div><div class="endnote" id="d25407e252"><span class="noteRef">[5]</span>
          See <a class="ref" href="http://www.perseus.tufts.edu/hopper" onclick="window.open('http://www.perseus.tufts.edu/hopper'); return false">http://www.perseus.tufts.edu/hopper/</a>.
        </div><div class="endnote" id="d25407e258"><span class="noteRef">[6]</span>See <a class="ref" href="http://www.tlg.uci.edu/" onclick="window.open('http://www.tlg.uci.edu/'); return false">http://www.tlg.uci.edu/</a>.
        </div><div class="endnote" id="d25407e316"><span class="noteRef">[7]</span>See [<a class="ref" href="#bourne1916">Bourne 1916</a>] for an overview of
          <em class="emph">puer</em> in <em class="emph">Ec.</em> IV.</div><div class="endnote" id="d25407e597"><span class="noteRef">[8]</span>At the time of writing, the SEMEVAL-1/SENSEVAL-4 (2007) competition is
            currently underway.</div><div class="endnote" id="d25407e852"><span class="noteRef">[9]</span>See [<a class="ref" href="#bamman2007">Bamman and Crane 2007</a>] for a summary of
            this work.</div><div class="endnote" id="d25407e879"><span class="noteRef">[10]</span>See <a class="ref" href="http://nlp.perseus.tufts.edu/syntax/treebank/" onclick="window.open('http://nlp.perseus.tufts.edu/syntax/treebank/'); return false">http://nlp.perseus.tufts.edu/syntax/treebank/</a>.
          </div></div><div id="worksCited"><h2>Works Cited</h2><div class="bibl fallback"><span class="ref" id="andrews1850"><!-- close -->Andrews 1850</span> Andrews, E. A. (ed.)<cite class="title">A Copious and Critical Latin-English Lexicon,
        Founded on the Larger Latin-German Lexicon of Dr. William Freund; With Additions and
        Corrections from the Lexicons of Gesner, Facciolati, Scheller, Georges, etc.</cite>.
        New York: Harper &amp; Bros.,
      1850.</div><div class="bibl fallback"><span class="ref" id="bamman2007"><!-- close -->Bamman and Crane 2007</span> Bamman, David and Gregory Crane. "The Latin Dependency
        Treebank in a Cultural Heritage Digital Library", <cite class="title">Proceedings of the ACL
        Workshop on Language Technology for Cultural Heritage Data</cite> (2007).</div><div class="bibl fallback"><span class="ref" id="bamman2008"><!-- close -->Bamman and Crane 2008</span> Bamman, David and Gregory Crane. "Building a Dynamic Lexicon from a Digital Library", <cite class="title">Proceedings of the 8th ACM/IEEE-CS Joint Conference 
on Digital Libraries.</cite> (2008).</div><div class="bibl fallback"><span class="ref" id="banerjee2002"><!-- close -->Banerjee and Pedersen 2002</span> Banerjee, Sid and Ted Pedersen. "An Adapted Lesk
        Algorithm for Word Sense Disambiguation Using WordNet", <cite class="title">Proceedings of the
        Conference on Computational Linguistics and Intelligent Text Processing</cite>
      (2002).</div><div class="bibl fallback"><span class="ref" id="bourne1916"><!-- close -->Bourne 1916</span> Bourne, Ella. "The Messianic Prophecy in Vergil’s Fourth
        Eclogue", <cite class="title">The Classical Journal</cite>11.7 (1916).</div><div class="bibl fallback"><span class="ref" id="brants2006"><!-- close -->Brants and Franz 2006</span> Brants, Thorsten and Alex Franz. <cite class="title">Web 1T 5-gram Version 1</cite>.
        Philadelphia: Linguistic Data Consortium,
        2006.</div><div class="bibl fallback"><span class="ref" id="brown1991c"><!-- close -->Brown et al. 1991c</span> Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della Pietra and Robert L.
      Mercer. "Word-sense disambiguation using statistical
      methods", <cite class="title">Proceedings of the 29th Conference of the Association for
        Computational Linguistics</cite> (1991).</div><div class="bibl fallback"><span class="ref" id="thomisticus"><!-- close -->Busa 1974-1980</span> Busa, Roberto. <cite class="title">Index Thomisticus: sancti Thomae Aquinatis operum
        omnium indices et concordantiae, in quibus verborum omnium et singulorum formae et lemmata
        cum suis frequentiis et contextibus variis modis referuntur quaeque / consociata plurium
        opera atque electronico IBM automato usus digessit Robertus Busa SI</cite>.
        Stuttgart-Bad Cannstatt: Frommann-Holzboog,
        1974-1980.</div><div class="bibl fallback"><span class="ref" id="busa2004"><!-- close -->Busa 2004</span> Busa, Roberto. "Foreword: Perspectives on the Digital
        Humanities", <cite class="title">Blackwell Companion to Digital Humanities</cite>.
        Oxford: Blackwell, 2004.</div><div class="bibl fallback"><span class="ref" id="charniak2000"><!-- close -->Charniak 2000</span> Charniak, Eugene. "A Maximum-Entropy-Inspired
      Parser", <cite class="title">Proceedings of NAACL</cite> (2000).</div><div class="bibl fallback"><span class="ref" id="collins1999"><!-- close -->Collins 1999</span> Collins, Michael. "Head-Driven Statistical Models for
        Natural Language Parsing", <cite class="title">Ph.D. thesis</cite>.
      Philadelphia: University of Pennsylvania,
        1999.</div><div class="bibl fallback"><span class="ref" id="freund1840"><!-- close -->Freund 1840</span> Freund, Wilhelm (ed.). <cite class="title">Wörterbuch der lateinischen Sprache: nach
        historisch-genetischen Principien, mit steter Berücksichtigung der Grammatik, Synonymik und
        Alterthumskunde</cite>. Leipzig: Teubner,
        1834-1840.</div><div class="bibl fallback"><span class="ref" id="gale1992a"><!-- close -->Gale et al. 1992a</span> Gale, William, Kenneth W. Church and David Yarowsky. "Using bilingual materials to develop word sense disambiguation methods",
        <cite class="title">Proceedings of the 4th International Conference on Theoretical and Methodological
        Issues in Machine Translation</cite> (1992).</div><div class="bibl fallback"><span class="ref" id="old"><!-- close -->Glare 1982</span> Glare, P. G. W. (ed.). <cite class="title">Oxford Latin Dictionary</cite>.
        Oxford: Oxford University Press,
      1968-1982.</div><div class="bibl fallback"><span class="ref" id="grozea2004"><!-- close -->Grozea 2004</span> Grozea, Christian. "Finding Optimal Parameter Settings
        for High Performance Word Sense Disambiguation", <cite class="title">Proceedings of Senseval-3:
        Third International Workshop on the Evaluation of Systems for the Semantic Analysis of
      Text</cite> (2004).</div><div class="bibl fallback"><span class="ref" id="hajic1999"><!-- close -->Hajič 1999</span> Hajič, Jan. "Building a Syntactically Annotated Corpus:
        The Prague Dependency Treebank", <cite class="title">Issues of Valency and Meaning. Studies in
        Honour of Jarmila Panevová</cite>. Prague: Charles
        University Press, 1999.</div><div class="bibl fallback"><span class="ref" id="kilgarriff2004"><!-- close -->Kilgarriff et al. 2004</span> Kilgarriff, Adam, Pavel Rychly, Pavel Smrz, and David Tugwell. "The Sketch Engine", <cite class="title">Proceedings of EURALEX</cite>
      (2004).</div><div class="bibl fallback"><span class="ref" id="klosa2006"><!-- close -->Klosa et al. 2006</span> Klosa, Annette, Ulrich Schnörch, and Petra Storjohann. "ELEXIKO — A Lexical and Lexicological, Corpus-based Hypertext Information System at the
        Institut für deutsche Sprache, Mannheim", <cite class="title">Proceedings of the 12th Euralex
        International Congress</cite> (2006).</div><div class="bibl fallback"><span class="ref" id="lesk1986"><!-- close -->Lesk 1986</span> Lesk, Michael. "Automatic Sense Disambiguation Using Machine Readable Dictionaries: How to Tell a Pine Cone
        from an Ice Cream Cone", <cite class="title">Proceedings of the ACM-SIGDOC Conference</cite>
        (1986).</div><div class="bibl fallback"><span class="ref" id="lewis1879"><!-- close -->Lewis and Short 1879</span> Lewis, Charles T. and Charles Short (eds.). <cite class="title">A Latin
      Dictionary</cite>. Oxford: Clarendon Press,
        1879.</div><div class="bibl fallback"><span class="ref" id="liddell1940"><!-- close -->Liddell and Scott 1940</span> Liddell, Henry George and Robert Scott (eds.). <cite class="title">A Greek-English
        Lexicon, revised and augmented throughout by Sir Henry Stuart Jones</cite>.
        Oxford: Clarendon Press, 1940.</div><div class="bibl fallback"><span class="ref" id="marcus1993b"><!-- close -->Marcus et al. 1993b</span> Marcus, Mitchell P., Beatrice Santorini, and Mary Ann Marcinkiewicz. "Building a Large Annotated Corpus of English: The Penn Treebank",
        <cite class="title">Computational Linguistics</cite>19.2 (1994).</div><div class="bibl fallback"><span class="ref" id="mccarthy2004"><!-- close -->McCarthy et al. 2004</span> McCarthy, Diana, Rob Koeling, Julie Weeds and John Carroll. "Finding Predominant Senses in Untagged Text", <cite class="title">Proceedings of the
        42nd Annual Meeting of the Association for Computational Linguistics</cite>
      (2004).</div><div class="bibl fallback"><span class="ref" id="mcdonald2005"><!-- close -->McDonald et al. 2005</span> McDonald, Ryan, Fernando Pereira, Kiril Ribarov, and Jan Hajič. "Non-projective Dependency Parsing using Spanning Tree Algorithms",
        <cite class="title">Proceedings of HLT/EMNLP</cite> (2005).</div><div class="bibl fallback"><span class="ref" id="melcuk1988"><!-- close -->Mel’čuk 1988</span> Mel’čuk, Igor A.<cite class="title">Dependency Syntax: Theory and Practice</cite>. Albany:
        State University of New York Press, 1988.</div><div class="bibl fallback"><span class="ref" id="mihalcea2004"><!-- close -->Mihalcea and Edmonds 2004</span> Mihalcea, Rada and Philip Edmonds (eds.). <cite class="title">Proceedings of Senseval-3:
        Third International Workshop on the Evaluation of Systems for the Semantic Analysis of
      Text</cite> (2004).</div><div class="bibl fallback"><span class="ref" id="wordnet"><!-- close -->Miller 1995</span> Miller, George. "Wordnet: A Lexical Database",
        <cite class="title">Communications of the ACM</cite>38.11 (1995).</div><div class="bibl fallback"><span class="ref" id="miller1993b"><!-- close -->Miller et al. 1993b</span> Miller, George, Claudia Leacock, Randee Tengi, and Ross Bunker. "A Semantic Concordance", <cite class="title">Proceedings of the ARPA Workshop on
        Human Language Technology</cite> (1993).</div><div class="bibl fallback"><span class="ref" id="moore2002"><!-- close -->Moore 2002</span> Moore, Robert C."Fast and Accurate Sentence Alignment of Bilingual Corpora",
        <cite class="title">AMTA '02: Proceedings of the 5th Conference of the Association for Machine
        Translation in the Americas on Machine Translation</cite> (2002).</div><div class="bibl fallback"><span class="ref" id="niermeyer1976"><!-- close -->Niermeyer 1976</span> Niermeyer, Jan Frederick. <cite class="title">Mediae Latinitatis Lexicon Minus</cite>.
        Leiden: Brill, 1976.</div><div class="bibl fallback"><span class="ref" id="nivre2006"><!-- close -->Nivre et al. 2006</span> Nivre, Joakim, Johan Hall, and Jens Nilsson. "MaltParser:
        A Data-Driven Parser-Generator for Dependency Parsing", <cite class="title">Proceedings of the
        Fifth International Conference on Language Resources and Evaluation</cite>
      (2006).</div><div class="bibl fallback"><span class="ref" id="och2003b"><!-- close -->Och and Ney 2003</span> Och, Franz Josef and Hermann Ney. "A Systematic
        Comparison of Various Statistical Alignment Models", <cite class="title">Computational Linguistics</cite>29.1 (2003).</div><div class="bibl fallback"><span class="ref" id="pinkster1990"><!-- close -->Pinkster 1990</span> Pinkster, Harm. <cite class="title">Latin Syntax and Semantics</cite>.
      London: Routledge, 1990.</div><div class="bibl fallback"><span class="ref" id="schutz1895"><!-- close -->Schütz 1895</span> Schütz, Ludwig. <cite class="title">Thomas-Lexikon</cite>.
      Paderborn: F. Schoningh, 1895.</div><div class="bibl fallback"><span class="ref" id="sinclair1987"><!-- close -->Sinclair 1987</span> Sinclair, John M. (ed.). <cite class="title">Looking Up: an account of the COBUILD project
        in lexical computing</cite>. Collins, 1987.</div><div class="bibl fallback"><span class="ref" id="singh2005"><!-- close -->Singh and Husain 2005</span> Singh, Anil Kumar and Samar Husain. "Comparison,
        Selection and Use of Sentence Alignment Algorithms for New Language Pairs",
        <cite class="title">Proceedings of the ACL Workshop on Building and Using Parallel Texts</cite>
        (2005).</div><div class="bibl fallback"><span class="ref" id="tll"><!-- close -->TLL</span> <cite class="title">Thesaurus Linguae Latinae, fourth electronic edition</cite>.
      Munich: K. G. Saur, 2006.</div><div class="bibl fallback"><span class="ref" id="tufis2004"><!-- close -->Tufis et al. 2004</span> Tufis, Dan, Radu Ion, and Nancy Ide. "Fine-Grained Word
        Sense Disambiguation Based on Parallel Corpora, Word Alignment, Word Clustering and Aligned
        Wordnets", <cite class="title">Proceedings of the 20th International Conference on Computational
        Linguistics </cite> (2004).</div></div><div class="toolbar"><a href="../index.html">2009 3.1</a>
             | 
            <a rel="external" href="000033.xml">XML</a>
            | 
            <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div></div><script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'dhquarterly'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script><div id="comments"><div id="disqus_thread"/><script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'dhquarterly'; // required: replace example with your forum shortname

    // The following are highly recommended additional parameters. Remove the slashes in front to use.
    var disqus_identifier = '000033';
    var disqus_url = '000033.html';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><div id="footer"> 
            URL: http://www.digitalhumanities.org/dhq/vol/3/1/000033/000033.html<br/>Last updated:
            <script type="text/javascript">
                var monthArray = new initArray("January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December");
                var lastModifiedDate = new Date(document.lastModified);
                var currentDate = new Date();
                document.write(" ",monthArray[(lastModifiedDate.getMonth()+1)]," ");
                document.write(lastModifiedDate.getDate(),", ",(lastModifiedDate.getFullYear()));
            </script><br/> Comments: <a href="mailto:dhqinfo@digitalhumanities.org" class="footer">dhqinfo@digitalhumanities.org</a><br/> Published by:
            <a href="http://www.digitalhumanities.org/" class="footer">The Alliance of Digital Humanities Organizations</a><br/>Affiliated with: <a href="http://llc.oxfordjournals.org/">Literary and Linguistic Computing</a><br/> Copyright 2005 - <script type="text/javascript">
                document.write(currentDate.getFullYear());</script><br/> This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/3.0/">Creative Commons
                    Attribution-Noncommercial-No Derivative Works 3.0 License</a><br/><a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/3.0/"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-nc-nd/3.0/88x31.png"/></a></div></div></div></body>
<!-- Mirrored from www.digitalhumanities.org/dhq/vol/3/1/000033/000033.html by HTTrack Website Copier/3.x [XR&CO'2010], Sat, 16 May 2015 13:18:40 GMT -->
</html>