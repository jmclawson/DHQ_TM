<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<!-- Mirrored from www.digitalhumanities.org/dhq/vol/2/1/000019/000019.html by HTTrack Website Copier/3.x [XR&CO'2010], Sat, 16 May 2015 13:18:49 GMT -->
<head><meta http-equiv="content-type" content="text/html; charset=utf-8"/><title>DHQ: Digital Humanities Quarterly: As You Can See: Applying Visual Collaborative Filtering to Works of Art</title><link rel="stylesheet" type="text/css" href="../../../../common/css/dhq.css"/><link rel="stylesheet" type="text/css" media="screen" href="../../../../common/css/dhq_screen.css"/><link rel="stylesheet" type="text/css" media="print" href="../../../../common/css/dhq_print.css"/><link rel="alternate" type="application/atom+xml" href="../../../../feed/news.xml"/><link rel="shortcut icon" href="../../../../common/images/favicon.ico"/><script type="text/javascript" src="../../../../common/js/javascriptLibrary.js">
                &lt;!-- Javascript functions --&gt;
            </script><script type="text/javascript">

 var _gaq = _gaq || [];
 _gaq.push(['_setAccount', 'UA-15812721-1']);
 _gaq.push(['_trackPageview']);

 (function() {
   var ga = document.createElement('script'); ga.type =
'text/javascript'; ga.async = true;
   ga.src = ('https:' == document.location.protocol ? 'https://ssl' :
'http://www') + '.google-analytics.com/ga.js';
   var s = document.getElementsByTagName('script')[0];
s.parentNode.insertBefore(ga, s);
 })();

        </script></head><body><div id="top"><div id="backgroundpic"><script type="text/javascript" src="../../../../common/js/pics.js"><!--displays banner image--></script></div><div id="banner"><div id="dhqlogo"><img src="http://www.digitalhumanities.org/dhq/common/images/dhqlogo.png" alt="DHQ Logo"/></div><div id="longdhqlogo"><img src="http://www.digitalhumanities.org/dhq/common/images/dhqlogolonger.png" alt="Digital Humanities Quarterly Logo"/></div></div><div id="topNavigation"><div id="topnavlinks"><span><a href="../../../../index.html" class="topnav">home</a></span><span><a href="../../../../submissions/index.html" class="topnav">submissions</a></span><span><a href="../../../../about/about.html" class="topnav">about dhq</a></span><span><a href="../../../../people/people.html" class="topnav">dhq people</a></span><span id="rightmost"><a href="../../../../contact/contact.html" class="topnav">contact</a></span></div><div id="search"><form action="http://www.digitalhumanities.org/dhq/findIt" method="get" onsubmit="javascript:document.location.href=cleanSearch(this.queryString.value); return false;"><div><input type="text" name="queryString" size="18"/> <input type="submit" value="Search"/></div></form></div></div></div><div id="main"><div id="leftsidebar"><div id="leftsidenav"><span>Current Issue<br/></span><ul><li><a href="../../../8/4/index.html">2014: 8.4</a></li></ul><span>Preview Issue<br/></span><ul><li><a href="../../../../preview/index.html">2015: 9.1</a></li></ul><span>Previous Issues<br/></span><ul><li><a href="../../../8/3/index.html">2014: 8.3</a></li><li><a href="../../../8/2/index.html">2014: 8.2</a></li><li><a href="../../../8/1/index.html">2014: 8.1</a></li><li><a href="../../../7/3/index.html">2013: 7.3</a></li><li><a href="../../../7/2/index.html">2013: 7.2</a></li><li><a href="../../../7/1/index.html">2013: 7.1</a></li><li><a href="../../../6/3/index.html">2012: 6.3</a></li><li><a href="../../../6/2/index.html">2012: 6.2</a></li><li><a href="../../../6/1/index.html">2012: 6.1</a></li><li><a href="../../../5/3/index.html">2011: 5.3</a></li><li><a href="../../../5/2/index.html">2011: 5.2</a></li><li><a href="../../../5/1/index.html">2011: 5.1</a></li><li><a href="../../../4/2/index.html">2010: 4.2</a></li><li><a href="../../../4/1/index.html">2010: 4.1</a></li><li><a href="../../../3/4/index.html">2009: 3.4</a></li><li><a href="../../../3/3/index.html">2009: 3.3</a></li><li><a href="../../../3/2/index.html">2009: 3.2</a></li><li><a href="../../../3/1/index.html">2009: 3.1</a></li><li><a href="../index.html">2008: 2.1</a></li><li><a href="../../../1/2/index.html">2007: 1.2</a></li><li><a href="../../../1/1/index.html">2007: 1.1</a></li></ul><span>Indexes<br/></span><ul><li><a href="../../../../index/title.html"> Title</a></li><li><a href="../../../../index/author.html"> Author</a></li></ul></div><img src="http://www.digitalhumanities.org/dhq/common/images/lbarrev.png" style="margin-left : 7px;" alt="sidenavbarimg"/><div id="leftsideID"><b>ISSN 1938-4122</b><br/></div><div class="leftsidecontent"><h3>Announcements</h3><ul><li><a href="../../../../announcements/index.html#reviewers">Call for Reviewers</a></li><li><a href="../../../../announcements/index.html#submissions">Call for Submissions</a></li></ul></div><div class="leftsidecontent"><script type="text/javascript">addthis_pub  = 'dhq';</script><a href="http://www.addthis.com/bookmark.php" onmouseover="return addthis_open(this, '', '[URL]', '[TITLE]')" onmouseout="addthis_close()" onclick="return addthis_sendto()"><img src="http://s9.addthis.com/button1-addthis.gif" width="125" height="16" alt="button1-addthis.gif"/></a><script type="text/javascript" src="../../../../../../s7.addthis.com/js/152/addthis_widget.js">&lt;!-- Javascript functions --&gt;</script></div></div><div id="mainContent"><div id="printSiteTitle">DHQ: Digital Humanities Quarterly</div><div xmlns:dhqBiblio="http://digitalhumanities.org/dhq/ns/biblio" class="DHQarticle"><div id="pubInfo">2008<br/>Volume 2 Number 1</div><div class="toolbar"><form id="taporware" action="http://www.digitalhumanities.org/dhq/vol/2/1/000019/get"><div><a href="../index.html">2008 2.1</a>
                     | 
                    <a rel="external" href="000019.xml">XML</a>

| 
		   Discuss
			(<a href="000019.html#disqus_thread" data-disqus-identifier="000019">
				Comments
			</a>)
                </div></form></div><div class="DHQheader"><h1 class="articleTitle">As You Can See: Applying Visual Collaborative Filtering to Works of Art</h1><div class="author"><a rel="external" href="../bios.html#nauta_g">Gerhard Jan Nauta</a> &lt;<a href="mailto:G_dot_J_dot_Nauta at let_dot_leidenuniv_dot_nl" onclick="javascript:window.location.href='mailto:'+deobfuscate('G_dot_J_dot_Nauta at let_dot_leidenuniv_dot_nl'); return false;" onkeypress="javascript:window.location.href='mailto:'+deobfuscate('G_dot_J_dot_Nauta at let_dot_leidenuniv_dot_nl'); return false;">G_dot_J_dot_Nauta at let_dot_leidenuniv_dot_nl</a>&gt;, Leiden University</div><span xmlns="" class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft.genre=article&amp;rft.atitle=As%20You%20Can%20See%3A%20Applying%20Visual%20Collaborative%20Filtering%20to%20Works%20of%20Art&amp;rft.jtitle=Digital%20Humanities%20Quarterly&amp;rft.stitle=DHQ&amp;rft.issn=1938-4122&amp;rft.date=2008-06-21&amp;rft.volume=002&amp;rft.issue=1&amp;rft.aulast=Nauta&amp;rft.aufirst=Gerhard Jan&amp;rft.au=Gerhard Jan%20Nauta"> </span></div><div id="DHQtext"><div id="abstract"><h2>Abstract</h2><p>Art historically relevant visual knowledge can be deconstructed and the resulting
                components of this visual knowledge — visual discernments — lend themselves to be
                socially negotiated. Individual visual experts (like connoisseurs) do not share some
                grand and undividable cognitive cataloguing system; they are attentive to piecemeal
                visual discernments and the patterns in which these occur in reality. In
                conventional scholarly communication sophisticated tools to discuss perceptual
                patterns are lacking. This paper not only proposes a theoretical model of visual
                knowledge accumulation, but also describes a practical implementation, <cite class="title italic">Art.Similarities</cite>, which is designed as a prototype of such
                a sophisticated tool. Using a custom-made interface it records visual behavior: the
                non-verbally expressed visual similarity judgments of distributed individuals. Users
                can be assigned to groups according to the qualities of their judgments. These
                qualities may be distilled from emerging similarity patterns. The implications of
                individual judgments in different user groups may vary considerably. Emerging
                patterns can be assessed both according to human analysis and statistical
                procedures. Most studies on art evaluation are attentive to either the
                characteristics of works, or the characteristics of observers. In this study both
                are considered as interdependent entities consistently. </p></div><div class="div div0"><h1 class="head">Introduction</h1><div class="counter"><a href="#p1">1</a></div><div class="ptext" id="p1">Without being aware of the underlying technologies, our behavior is frequently
                recorded, assessed, and fed back to us via today's Web interfaces. Information
                systems are now far more advanced in mirroring preferences, curiosities, and yes,
                    <em class="emph">knowledge</em>, than they were, say, five years ago. The example which
                is familiar to us all is Amazon.com.<a class="noteRef" href="#d6604e85">[1]</a> The world's largest bookstore makes clever use of the newest
                collaborative filtering technology.<a class="noteRef" href="#d6604e103">[2]</a> From a complex database of recorded consumer behavior persuasive,
                personalized web pages are generated. The idea behind the process is that
                statistically combining traces of user behavior will yield knowledge about user
                    interests.<a class="noteRef" href="#d6604e117">[3]</a> If you have ever ordered a book that was
                suggested to you by Amazon, that forms the best proof of the success of the
                collaborative filtering approach.</div><div class="counter"><a href="#p2">2</a></div><div class="ptext" id="p2">Looking at the basic idea behind collaborative filtering systems, one may wonder
                whether this concept is applicable to the humanities, where the expression of value
                judgments (as against stating incontrovertible facts) is of such importance.
                Building a collaborative filtering tool involves first establishing what are
                supposed to be relevant data, and then modeling the way these data may be recorded
                and organized, in order to turn data into information, and perhaps collaborative knowledge.<a class="noteRef" href="#d6604e122">[4]</a></div><div class="counter"><a href="#p3">3</a></div><div class="ptext" id="p3">This paper discusses a field test in collaboratively establishing visual
                similarities, a topic of concern in art history. <cite class="title italic">Art.Similarities</cite> is the experimental application used to sample visual
                similarity judgments.<a class="noteRef" href="#d6604e136">[5]</a> The experiment could be relevant for other
                disciplines than the history of art, both inside and outside the humanities. </div><div class="counter"><a href="#p4">4</a></div><div class="ptext" id="p4">The bulk of research in distributed cognition is text-based — as we shall see in the
                next paragraph. For this type of research in information science and cognitive
                psychology an example in the domain of visual cognition may be advantageous. In
                particular crossing off the intermediary function of verbal language, may encourage
                the development of scenarios for combining transactional data (partial observations
                of visual artifacts) to construct emergent knowledge (holistic representations).
                This is why I believe that in its consequences this field test could support
                cross-cultural comparisons in visual cognition.</div><div class="counter"><a href="#p5">5</a></div><div class="ptext" id="p5">I also expect that the project may shine a new light on the relation between the
                cultural interpretation of images and words on one side, and the relation between
                such cultural interpretations and objective image data, e.g. as produced by
                content-based image retrieval technologies [<a class="ref" href="#eakins1999">Eakins &amp; Graham 1999</a>], on the other
                side. In other words: it could help tracing the line between objective visual facts
                (as captured for instance in a color histogram) and more interpretive
                "truths". </div><div class="counter"><a href="#p6">6</a></div><div class="ptext" id="p6">In the practical sphere, because visual attention becomes partly measurable and
                comparable, the project may contribute to the development of educational
                applications, in which students learn to reflect on culture as a determinant of
                visual cognition. And because the interconnectedness of base distinctions and more
                evolved cultural knowledge is worked out computationally, it becomes conceivable
                that by means of technology subjects will be offered the tools to vary the
                compositional elements of a visual configuration, just as when you add or remove
                query strings based on intermediate search results, when searching large volumes of
                texts.</div><div class="counter"><a href="#p7">7</a></div><div class="ptext" id="p7">I presume that the latent possibilities of the <cite class="title italic">Art.Similarities</cite> experiment will only come to the fore when we do not
                discuss it in isolation. Therefore consider in advance that interesting alternatives
                may evolve when varying such dimensions as:</div><div class="ptext"><ul class="list"><li class="item">the scale on which the application is used (small, maybe local communities
                    versus large, global crowds); </li><li class="item">a focus on gathering coherent large scale data versus a focus on developing
                    various applications (to be populated with different sets of data); </li><li class="item">the application in isolation versus the application as a building block in
                    more complicated dedicated software (e.g. as one of the options to organize
                    visual materials in image databases or image processing software).</li></ul></div><div class="counter"><a href="#p8">8</a></div><div class="ptext" id="p8">Such dimensions are important in assessing the usefulness of the experiment. Just to
                get an idea of the kind of problem addressed here, consider these two works of art,
                dating from the early 1930s:</div><div id="figure01" class="figure"><div class="ptext"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/figure01.jpg" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/figure01.jpg" alt=""/></a></div><div class="caption"><div class="label">Figure 1. </div>Georgia O'Keeffe, <cite class="title italic">Black and White</cite>, 1930, oil
                    on canvas. Whitney Museum of American Art, New York. © c/o Pictoright Amsterdam
                    2008.</div></div><div id="figure02" class="figure"><div class="ptext"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/figure02.jpg" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/figure02.jpg" alt=""/></a></div><div class="caption"><div class="label">Figure 2. </div>Edward Weston, <cite class="title italic">Sand Dunes, Oceano</cite>, 1934,
                    gelatin silver print. Center for Creative Photography, Tucson AZ. © c/o CCP
                    Tucson AZ 2008</div></div><div class="counter"><a href="#p9">9</a></div><div class="ptext" id="p9">Both images show a striking overall similarity. Experienced observers seeing these
                works of art, may draw conclusions about artistic influences. And indeed, artists
                are observers by profession, adopting typical ways of expressing visual thoughts in
                new visual configurations. The history of art has documented innumerable instances
                of both explicit and implicit borrowing of formal traits and configurations.<a class="noteRef" href="#d6604e201">[6]</a> This is
                partly motivated by the common assumption that visual phenomena in cultural
                artifacts are indices of material, personal and social conditions during the time of
                creation.</div><div class="counter"><a href="#p10">10</a></div><div class="ptext" id="p10">Establishing visual similarities is part of the groundwork in the field of art
                history, but making these similarity observations verifiable is precisely where the
                conventional systems of scholarly communication fail, since neither do we have an
                effective way of discussing visual similarities, nor do we have the means to trace
                observed similarities in large collections of images.<a class="noteRef" href="#d6604e208">[7]</a></div></div><div class="div div0"><h1 class="head">Collaborative Knowledge</h1><div class="counter"><a href="#p11">11</a></div><div class="ptext" id="p11">In recent years we have seen several attempts at constructing frameworks for the
                study of collaborative knowledge. The subject was variously denoted with terms such
                as "distributed cognition", "social cognition",
                    "situated cognition", or "group cognition". The
                urge to develop theory in this field has increased dramatically since the waves of
                hypertext enthusiasm in the late 1980s and early 1990s, the subsequent rise of the
                world wide web and later semantic web, and the popularization, recently, of social
                software, of which the collaborative filtering systems mentioned earlier are an
                omnipresent example. In theories of collaborative knowledge the psychology of Lev
                Vygotsky, who may have been one of the first to envision a socially constructed
                mind, is frequently referred to. His <cite class="title italic">Mind in Society</cite>
                dates back to 1930 [<a class="ref" href="#vygotsky1976">Vygotsky 1976</a>]. Gavriel Salomon stressed the
                importance of the complementary concepts of "shared cognition"
                (knowledge of the world which is continuously established through live interactions
                among individuals) and "off-loaded cognition" (recording and
                processing cognitive facts and functions to realia, such as concept maps) [<a class="ref" href="#salomon1996">Salomon 1996</a>]. In 1995 Edwin Hutchins published another influential
                book, <cite class="title italic">Cognition in the Wild</cite>, where the author uses the
                metaphor of ship navigation to pinpoint the cultural nature of cognition: no single
                individual from the ship's crew is capable of managing all the complex operations
                that are necessary to sail [<a class="ref" href="#hutchins1995">Hutchins 1995</a>].</div><div class="counter"><a href="#p12">12</a></div><div class="ptext" id="p12">Important building blocks of frameworks for collaborative knowledge are: thought
                processes, as distributed amongst a group of individuals, representations of these
                thought processes, as captured in external realia, and mediating processes (i.e.
                computer systems) that are capable of coordinating both internal and external
                representations and lifting the newly generated forms of knowledge above the level
                of consciousness of the individual. </div><div class="counter"><a href="#p13">13</a></div><div class="ptext" id="p13">A recent attempt to theorize in this field is Gerry Stahl’s book <cite class="title italic">Group Cognition: Computer Support for Building Collaborative Knowledge</cite>[<a class="ref" href="#stahl2006">Stahl 2006</a>]. On the basis of a series of experiments, in which the
                promise of computer supported knowledge negotiation has been tested, Stahl analyses
                the mechanisms of collaborative knowledge building and ends by presenting a
                tentative theory of group cognition. In Stahl’s model the sphere of personal
                understanding is opposed to and merges with a cycle of social knowledge building.
                Individuals articulate – in public statements – personal perspectives, which in a
                process of argumentation and rationale are being assimilated with the perspectives
                of other individuals into collaborative knowledge. This collaborative knowledge is
                formalized and objectified in cultural artifacts that can be observed and known by
                individuals. Et cetera. Stahl’s model may help pointing out the stages of the
                process of knowledge construction and thus facilitate the design of specific forms
                of computer support [<a class="ref" href="#stahl2006">Stahl 2006</a>, 207].</div><div class="counter"><a href="#p14">14</a></div><div class="ptext" id="p14">Typical for Stahl’s approach is his focus on verbal discourse. Although in some of
                his experiments schematic representations of e.g. floor plans or network
                architectures are crucial to the tasks of his subjects, in most of the experiments
                he uses chat-like conversations and threaded discussions to unfold his theory. But
                the kind of knowledge being researched here is visual knowledge constructed by
                visual means. As yet I have found no models of distributed cognition covering this
                particular domain. In my model the visual discernments ("personal
                focus") of individuals are expressed ("off-loaded") in visual
                images, where the computer records and mediates ("negotiation of
                perspectives").</div></div><div class="div div0"><h1 class="head">Basic Assumptions</h1><div class="counter"><a href="#p15">15</a></div><div class="ptext" id="p15">It is taken for granted in this study that the visual appearance of a cultural
                artifact, and thus its potential similarity to other artifacts, is resolvable into
                literally innumerable formal qualities. Two artifacts may share any number of formal
                qualities, and these are thought to somehow account for their similarity. Two
                (visually) identical (=maximally similar) artifacts share immense quantities of
                visual characteristics, whereas for two very dissimilar artifacts the number of
                shared visual traits is minimal. In between the extremes the rule would be that
                    <em class="emph">the more two cultural artifacts share a set of singular formal/visual
                    attributes/traits, the more they are experienced as being similar to one
                    another.</em><a class="noteRef" href="#d6604e286">[8]</a> In other words: the actually perceived similarity is
                defined as the function of an unspecified number of trait-to-trait similarities. </div><div class="counter"><a href="#p16">16</a></div><div class="ptext" id="p16">In order to avoid endless image dissections, another assumption adopted here is that
                the overall visual appearance of a cultural artifact in fact approximates to the
                weighted sum of relatively few, but <em class="emph">distinctive</em> formal/visual
                attributes/traits. So I assume that culture is a decisive factor in subjects
                focusing on specific image characteristics: the threat of an infinite number of
                physical features is being balanced by cultural determinants.<a class="noteRef" href="#d6604e296">[9]</a></div><div class="counter"><a href="#p17">17</a></div><div class="ptext" id="p17">Observers have a number of options to denote a visual concept [<a class="ref" href="#nauta2001">Nauta 2001</a>]. In most of these options abstract symbols (words,
                numbers, etc.) signify visual similarities. Two options, however, concern iconic
                references. One focuses on the simultaneous display of multiple instances of similar
                artifacts along one or several specific attribute(s). For instance, Santini, Gupta
                and Jain discuss an experimental interface in which the user is allowed "to manipulate not only the individual images, but also the
                        relation between them" [<a class="ref" href="#santini2001">Santini et al. 2001</a>]. An example of fairly successful content-based image retrieval is the system
                developed by D.P. Huijsmans et al. for the <cite class="title italic">Leiden 19th-Century
                    Portrait Database</cite> (LCPD), a database of Dutch carte-de-visite studio
                portraits (1860-1914). One of the options for consulting the LCPD is a so-called
                    "relevance feedback" interface. After each retrieval action the
                user is presented a table of results. Feedback is given by just clicking the
                photographs that are close to what the user was looking for, whereupon the system —
                using pre-calculated image characteristics — offers a new set of images, ideally
                closer matching the user's preferences. Et cetera. [<a class="ref" href="#huijsmans1999">Huijsmans &amp; Smeulders 1999</a>] In
                the other type of iconic denotation, the one elaborated here, image similarities are
                indicated by simple images exemplifying a restricted set of formal
                    attributes.<a class="noteRef" href="#d6604e318">[10]</a> These simple images are thus conceived as
                visual denotats.<a class="noteRef" href="#d6604e325">[11]</a> So where Santini et al. avoid
                immediate deconstruction of holistic appearances, this paper will explicitly
                    <em class="emph">analyze</em> similarities in considering <em class="emph">iconic</em>
                    denotations.<a class="noteRef" href="#d6604e340">[12]</a> In this way, problematic verbal denomination
                issues will be evaded. Present-day computers are perfectly suited to create systems
                for communicating pronouncements on matters of visual similarities through instant
                presentation of instances.<a class="noteRef" href="#d6604e364">[13]</a></div><div class="counter"><a href="#p18">18</a></div><div class="ptext" id="p18">Another assumption concerning the similarity of artifacts has to do with the quantity
                of common visual traits. Where two visual artifacts share a substantial number of
                different visual traits, we say that these artifacts manifest <em class="emph">multi-trait
                    similarity</em>. The notion of "style" may be akin to this
                concept. In the vocabulary of this article style could be defined as
                    <em class="emph">multi-trait similarity occurring across multiple images.</em></div><div class="counter"><a href="#p19">19</a></div><div class="ptext" id="p19">Apart from considering the number of visual traits two artifacts have in common, it
                is important to recognize that the <em class="emph">relative weight</em> of these visual
                traits is of interest: the more observers indicate they have discerned one
                particular trait, the more relevant this trait will be in any function defining the
                visual similarity of the artifact to any other artifact. Of course it should be kept
                in mind that relevance is dependent on the <em class="emph">group</em> these observers belong
                to. We are <em class="emph">not</em> considering facts. We need individual human observers to
                assess the visual qualities of artifacts. And assessments may either apply to the
                entire surface of a visual artifact or to only a segment of that surface. Once these
                assessments are recorded they can be analyzed. And the more individuals are involved
                in assessing visual qualities, the greater the cultural relevance of these
                assessments will be. Patterns in individual assessments, <em class="emph">calculated</em> for
                social groups, may reveal "high order" notions (such as — perhaps —
                    <em class="emph">stylistic notions)</em>. That an individual observer giving individual
                assessments need not be conscious of these high order notions is a fascinating
            idea.</div></div><div class="div div0"><h1 class="head">Hypothesis</h1><div class="counter"><a href="#p20">20</a></div><div class="ptext" id="p20">The following hypothesis aims at focusing the points made so far: The social formation of opinions concerning the visual similarity of cultural
                artifacts amongst a group of motivated observers can be modeled and recorded
                    <em class="emph">without</em> the application of verbal descriptors, using information
                technology (web technology, databases, statistics), in such a way as to make real
                use of archived records for purposes like: getting to know broad classes of image
                attributes; retrieving related images; and learning the preferences and perceptual
                biases of user classes and individual users.</div></div><div class="div div0"><h1 class="head">Experimental Setup</h1><div class="counter"><a href="#p21">21</a></div><div class="ptext" id="p21">In order to test the hypothesis an experimental application — <cite class="title italic">Art.Similarities</cite> — had to be developed, offering access to a modest
                quantity of digital images. The application interface had to be constructed in such
                a way that it would be relatively easy for observers to link visual icons from a
                restricted but varied set to the digital objects in the collection. Every
                transaction involving the database would have to be recorded to be able to
                quantitatively compare the visual discernments of different observers.</div><div class="counter"><a href="#p22">22</a></div><div class="ptext" id="p22">Here is an approximation of the processes the application should ideally support:</div><div class="ptext"><ol class="list"><li class="item">Users identify themselves. New users may be asked to submit a
                        <em class="emph">preprocessing user profile</em>, containing data such as: name, age,
                    sex, professional affiliation, etc. Returning users just log in. </li><li class="item">Novice users are assigned Level 1 privileges, meaning that they are allowed to
                    assess single trait similarities (artifact-icon associations; explained below). </li><li class="item">A sequence of artifacts is being displayed, one by one, in random order. The
                    Level 1 user is invited to select an appealing work. </li><li class="item">The selected artifact is presented together with a set of uniform, but
                    visually maximally diverse icons. </li><li class="item">Level 1 users distinguish single trait similarities between the simple (
                        "attenuated") images (icons) and ("replete") artifact reproductions, and fix these
                    similarities by means of icon-artifact attributions (one by one).<a class="noteRef" href="#d6604e454">[14]</a></li><li class="item">Attribution data are recorded, together with user IDs, date stamps, etc. </li><li class="item">The preceding steps (i.e. the attribution process) can be repeated, until the
                    user indicates he/she wishes to stop interactions. </li><li class="item">Since the artifact similarity measures are precisely quantified, statistical
                    procedures may be applied to the records in the database. This will yield
                        <em class="emph">artifact profiles</em>. </li><li class="item">Over time, artifact profiles will be generated/recorded, consisting of sets of
                    icon references related to individual images, together with user IDs. </li><li class="item">Artifact profiles may be displayed. Repeated artifact-icon associations will
                    become apparent immediately. </li><li class="item">Artifact similarity measures may be deduced from artifact profiles. These
                    similarities are either single trait (t=1) or multiple trait (t=N) similarities. </li><li class="item">Based on similarity measures the system can retrieve and display artifacts
                        "like" the one the user started with (~similarity-groups;
                    s-groups). </li><li class="item">The profile (in terms icons referred to) of any one of the retrieved artifacts
                    can be inspected individually. This way the user will be able to examine
                    multiple formal perspectives relevant for a given artifact. </li><li class="item">The user must be able to ask for recorded data concerning each and every
                    artifact-icon association.</li><li class="item">The more artifact-icon assignments a user makes, the more precise a <em class="emph">post
                        processing user profile</em> will freeze his visual cognition (behavior). </li><li class="item">Following a preconceived algorithm the coherence of both preprocessing and
                    post processing user profiles may be analyzed. The outcome may be fed into a
                    process of user classification. </li><li class="item">Either based on the preprocessing user profile only, or based on the analysis
                    just mentioned, some users may gain the status of Level 2 users (~superpeers). </li><li class="item">Level 2 users assess high-order artifact qualifications (profiles), thus
                    either implicitly or explicitly approving both perceived inter-artifact
                    similarities and Level 1 user articulations. </li></ol></div></div><div class="div div0"><h1 class="head">Technical Elaboration</h1><div class="div div1"><h2 class="head">Object Modeling and Database Design</h2><div class="counter"><a href="#p23">23</a></div><div class="ptext" id="p23">Although the combinatorial nature of calculating visual similarities may
                    eventually lead to a data explosion, the actual number of object classes in this
                    experiment is limited. There are <em class="emph">users/observers</em>, <em class="emph">visual
                        artifacts</em>, and <em class="emph">votes</em>, expressed by means of icons that
                    are indicative of perceived visual traits.<a class="noteRef" href="#d6604e520">[15]</a> For the object types in the experiment open
                    standards were adhered to wherever possible: </div><div class="ptext"><ul class="list"><li class="item">Visual <em class="emph">artifacts</em> were catalogued using the CDWA metadata
                        scheme [<a class="ref" href="#baca2005">Baca &amp; Harpring 2005</a>]. Among the attributes in the database are
                        creator name, title, date, etc. In the class diagram below only a subset of
                        the full set has been incorporated. </li><li class="item"><em class="emph">Users/observers</em> were classified according to common
                        attributes like name and other personal data such as address, gender, age,
                        etc. Specific attributes indicative of observer classes are: education,
                        profession and expertise.<a class="noteRef" href="#d6604e538">[16]</a></li><li class="item">To keep it simple <em class="emph">votes</em> were initially recorded with only a
                        minimum of attributes: a key referring to one of the icons, references to
                        the table of users (user name) and artifacts (object id), and a date (and
                        time) stamp.<a class="noteRef" href="#d6604e548">[17]</a></li></ul></div><div class="counter"><a href="#p24">24</a></div><div class="ptext" id="p24">The basic design of the database consisted of only three related tables. Two of
                    these (the <em class="emph">artifacts</em> and <em class="emph">votes</em> table, see below) were
                    so arranged as to be able to log all transactions. The third table (the
                        <em class="emph">user</em> table) serves the purpose of identifying specific users
                    and — in the next phase — recording the results of an analysis of performance
                        data.<a class="noteRef" href="#d6604e561">[18]</a></div><div class="counter"><a href="#p25">25</a></div><div class="ptext" id="p25">Figure 3 shows a class diagram of the system:</div><div id="figure03" class="figure"><div class="ptext"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/figure03.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/figure03.gif" alt=""/></a></div></div></div><div class="div div1"><h2 class="head">Interface Design</h2><div class="counter"><a href="#p26">26</a></div><div class="ptext" id="p26">In this experimental pilot the interface was designed as straightforward as
                    possible, with a firm eye on both database design and the basic tasks the users
                    of the application should be able to perform. As to these tasks the decision has
                    been to keep two different activities clearly separated: editing and
                    browsing/searching. These activities were reflected in two different interface
                    modes: <em class="emph">editing mode</em> and <em class="emph">serendipity mode</em>. The design
                    of these modes was based on slightly different principles. Switching from one
                    mode to the other, however, had to be self-explanatory. Within each of the modes
                    media objects, metadata, and interaction controls were placed within one and the
                    same screen (no overlapping windows). Still in both modes a clear balance was
                    sought between the object/artifact of the user's present focus/attention and
                    images for comparison, whether these would be simple icons or replete
                    (thumbnails of) artifacts.</div><div class="counter"><a href="#p27">27</a></div><div class="ptext" id="p27">Textual data were kept to a minimum. For navigation of the data collection the
                    decision was to rely on either <em class="emph">random presentations</em> (e.g. on
                    starting an interactive session) or <em class="emph">pointing and clicking</em>. Although
                    the underlying database of artifacts did in fact enable it, no textual search
                    options were offered. Since throughout a session the identity of the user should
                    be known, the first step of working with the program was authentication.</div><div class="counter"><a href="#p28">28</a></div><div class="ptext" id="p28">Here are two screendumps from <cite class="title italic">Art.Similarities</cite>:</div><div id="figure04" class="figure"><div class="ptext"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/figure04.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/figure04.gif" alt=""/></a></div><div class="caption"><div class="label">Figure 4. </div>Editing mode: Together with a selected work of art, a collection of
                        icons is on display. The task at hand is to check out the icon representing
                        the most outstanding visual trait of the artifact, by simply clicking one of
                        the icons. The association of artifact and icon is stored in the
                    database.</div></div><div id="figure05" class="figure"><div class="ptext"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/figure05.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/figure05.gif" alt=""/></a></div><div class="caption"><div class="label">Figure 5. </div>Serendipity mode: By clicking any of the icons assigned to a particular
                        artifact (left) the application will display an overview of other artifacts
                        to which this particular icon has been attributed. In this example the
                        listing is sorted according to one of the values of the descriptive
                        metadata. An alternative would be sorting according to the number of times a
                        particular icon has been assigned to the individual entities.</div></div></div><div class="div div1"><h2 class="head">Implementation</h2><div class="counter"><a href="#p29">29</a></div><div class="ptext" id="p29">For the population of the artifact file a collection of about 400 reproductions
                    of 2D artifacts and matching descriptive metadata was imported into the
                    database. This special purpose data set was compiled to have a research
                    collection, with a variety of provenances, media types, and formal
                    characteristics. The controlled addition of test pairs of similar images was
                    considered, but to avoid complexity it was decided to elaborate on this line of
                    thought in a follow-up.<a class="noteRef" href="#d6604e608">[19]</a></div><div class="counter"><a href="#p30">30</a></div><div class="ptext" id="p30">For pragmatic reasons in the experimental setup a more or less justifiable set of
                    icons was used. Students produced icons in dedicated assignments; others were
                    created by the author or were found on the Internet. Undoubtedly this will bring
                    some bias to the experiment. I will come back to the selection of indexing items
                    in the final section.</div><div class="counter"><a href="#p31">31</a></div><div class="ptext" id="p31">The number and size of icons had to be decided upon as well. Since for some time
                    to come the major impediment to rich visual comparisons will be the size and
                    resolution of display devices the use of large sets of icons of considerable
                    pixel dimensions, would go at the expense of the space available for the
                    presentation of the artifact under consideration. Since the simultaneous display
                    of both icons and artifact was considered desirable, the provisional layout was
                    fixed at 1 artifact and 100 icons.</div></div><div class="div div1"><h2 class="head">Use</h2><div class="counter"><a href="#p32">32</a></div><div class="ptext" id="p32">In five subsequent years (2003-2007) BA students in the history of art tested the
                        <cite class="title italic">Art.Similarities</cite> application. Students could
                    approach the system from any PC connected to the Internet. Some 4000
                    interactions (icon-artifact pairs) on a total of about 200 users were logged, so
                    the average number of indexing results per user in this pilot is about 20. Since
                    the design philosophy was to make the application as simple as possible, no help
                    function was offered. And indeed, students had no difficulties in using the
                    program. The analysis below is based on the database logs of this pilot. Some
                    evidence comes from an additional paper survey. </div></div></div><div class="div div0"><h1 class="head">An Analysis of Results</h1><div class="counter"><a href="#p33">33</a></div><div class="ptext" id="p33">Since the experiment considers three object types — observers, artifacts, icons — and
                since these object types are systematically interrelated — observers assign icons to
                works of art — the analysis may extend into at least three fields of semantic
                relevance. The primary concern is with how artifacts will be typified or defined as
                being more or less similar to one another. But an analysis of patterns in observer
                behavior and the covariance of icon-image assignments can equally yield interesting
                results.</div><div class="counter"><a href="#p34">34</a></div><div class="ptext" id="p34">Before we turn to an inspection of the collected data, however, a few words must be
                added on the representativeness of the sample. The following analysis of data is
                explorative. In comparison with today’s large-scale commercial data warehouses the
                couple of thousand of records in my transaction database is very small. In the real
                world data mining techniques will not be used on data sets of this size. Add to this
                that data were not collected systematically: subjects were not asked to visually tag
                a fixed number of artifacts; they were free to start or stop tagging any time, and
                artifacts were presented at random. The only choice the subjects had was to skip an
                artifact and go on with the next object displayed. As a result the choice of
                artifacts and the distribution of visual tags across the objects represented in the
                database cannot be fully representative. Still it was decided not to normalize data
                routinely. The meaning of differing tagging frequencies could be relevant. Below the
                results of the <cite class="title italic">Art.Similarities</cite> experiment will mostly
                be explored on the basis of raw data, especially where the similarity of artifacts
                is under consideration.</div><div class="counter"><a href="#p35">35</a></div><div class="ptext" id="p35">The setup of the test application is fairly simple. Nevertheless an analysis of
                database logs already requires substantial pre-processing and the application of
                sophisticated algorithms to extract measures of central tendency from the rapidly
                growing files. Just to compare one artifact and its related icons with all other
                artifacts, one needs to know both how many icons any pair of artifacts has in
                common, and the number of times any shared icon has been assigned to both artifacts.
                Below are some indications of what lies hidden in these database tables. My approach
                is a combination of searching for patterns in the data and doing empirical checks,
                in particular in the form of a visual assessment of artifacts and icons. Other
                methods of relating data and reality could be asking subjects (experts) to assess
                the results of manipulating transaction data, or even in some cases using
                content-based image retrieval technology to evaluate matches of artifacts.</div><div class="div div1"><h2 class="head">Artifact Characterizations</h2><div class="counter"><a href="#p36">36</a></div><div class="ptext" id="p36">To control for bias caused by low numbers, most of the analyses of image
                    similarity was done on the top 100 artifacts according to tagging frequencies.
                    This subset provided for 1834 (out of 3917) tags, so the average number of tags
                    per artifact was about 18. The co-occurrences for this dataset were computed
                    (n=28100). Starting from the table of co-occurrences a number of measures could
                    be extracted, the most immediate being the absolute number of co-occurrences per
                    pair of artifacts. This was taken as a rough measure of artifact similarity.</div><div class="div div2"><h3 class="head">Similarity Based on Absolute Numbers</h3><div class="counter"><a href="#p37">37</a></div><div class="ptext" id="p37">An empirical (visual) inspection of the top 25 of similar pairs according to
                        the absolute number of co-occurrences yielded no surprising results. The
                        visual characteristics corresponding to computed similarities were quite
                        obvious: strong primary colors (red, yellow, etc.), peculiar shapes (whirls,
                        extreme converging and/or parallel lines), or overall organizational
                        configurations (rectangular subdivisions, compositions balanced around two
                        or three main compositional elements). As stated above the subjects in this
                        experiment were free to skip artifacts in the process of tagging, so they
                        probably preferred obvious image characteristics. Frequencies of tag scores
                        — number of tags, number of co-occurrences, and number of associated
                        artifacts — seem to support that interpretation. Below I will have a closer
                        look at this.</div><div class="counter"><a href="#p38">38</a></div><div class="ptext" id="p38">The relative importance of vortex-like shapes was remarkable. Two
                        post-impressionist paintings — the <cite class="title italic">Starry Night</cite>
                        by Vincent van Gogh (1889) and <cite class="title italic">Portrait of Felix
                        Feneon</cite> by Paul Signac (1890-91), both in the Museum of Modern Art,
                        New York, co-occurred as much as 270 times. These works appeared in the top
                        of the list in association with another rather non-ambiguous
                        "whirly" image: a picture by fractal artist Forest Kenton
                        Musgrave (aka "Doc Mojo"): <cite class="title italic">Fractal Berry
                            01</cite> (1997). The Van Gogh painting also appeared in the top of the
                        list related to a photograph of a platoon of cyclists, driving along a
                        winding road in the Alps, and to another painting by Van Gogh, <cite class="title italic">Cornfield (with cypresses)</cite> (1889), now in the
                        Bürle Collection, Zürich. Maybe vortex-like configurations are eye-catching,<a class="noteRef" href="#d6604e678">[20]</a> though an alternative explanation
                        could be that the vocabulary in use (i.e. the set of 100 icons) was
                        particularly suited to tag vortex-like shapes. A possible <em class="emph">vocabulary
                            bias</em> must be taken into account.</div><div class="counter"><a href="#p39">39</a></div><div class="ptext" id="p39">Here is an example of two visually "similar" artifacts,
                        according to the raw database logs:</div><div id="figure06" class="figure"><div class="ptext"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/figure06.jpg" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/figure06.jpg" alt=""/></a></div><div class="caption"><div class="label">Figure 6. </div>Raphael (Raffaello Sanzio), <cite class="title italic">School of
                            Athens</cite>, between 1509-1511, fresco. Vatican (Stanza della
                            Segnatura), Rome.</div></div><div id="figure07" class="figure"><div class="ptext"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/figure07.jpg" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/figure07.jpg" alt=""/></a></div><div class="caption"><div class="label">Figure 7. </div>Marcel Molle, <cite class="title italic">Dean Hamer (rechts) en Steven
                                Rose in Amsterdam</cite>, 1999, photograph (De Volkskrant, June 12,
                            1999). © Marcel Molle Amsterdam.</div></div><div class="counter"><a href="#p40">40</a></div><div class="ptext" id="p40">The number of icon-artifact associations for this pair was distributed like
                        this:</div><div id="table01" class="table"><table class="table"><tr class="row"><td valign="top" class="cell"/><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i01.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i01.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i02.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i02.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i03.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i03.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i04.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i04.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i05.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i05.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i06.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i06.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i07.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i07.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i08.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i08.gif" alt=""/></a></td><td valign="top" class="cell"/></tr><tr class="row"><td valign="top" class="cell">Raphael, <cite class="title italic">School of Athens</cite></td><td valign="top" class="cell">8</td><td valign="top" class="cell">5</td><td valign="top" class="cell">1</td><td valign="top" class="cell">1</td><td valign="top" class="cell">0</td><td valign="top" class="cell">1</td><td valign="top" class="cell">1</td><td valign="top" class="cell">1</td><td valign="top" class="cell">18</td></tr><tr class="row"><td valign="top" class="cell">Molle, <cite class="title italic">Dean Hammer</cite></td><td valign="top" class="cell">7</td><td valign="top" class="cell">2</td><td valign="top" class="cell">2</td><td valign="top" class="cell">1</td><td valign="top" class="cell">1</td><td valign="top" class="cell">0</td><td valign="top" class="cell">0</td><td valign="top" class="cell">0</td><td valign="top" class="cell">13</td></tr><tr class="row"><td valign="top" class="cell"/><td valign="top" class="cell">15</td><td valign="top" class="cell">7</td><td valign="top" class="cell">3</td><td valign="top" class="cell">2</td><td valign="top" class="cell">1</td><td valign="top" class="cell">1</td><td valign="top" class="cell">1</td><td valign="top" class="cell">1</td><td valign="top" class="cell">31</td></tr></table><div class="caption-no-label"><div class="label">Table 1. </div></div></div><div class="counter"><a href="#p41">41</a></div><div class="ptext" id="p41">The number of co-occurrences computed for these artifacts was 69. The
                        distribution of icons shows that the Raphael painting has been tagged more
                        often. Normalization would have given a different ranking for this pair of
                        artifacts.</div></div><div class="div div2"><h3 class="head">Similarity Based on Weighted Co-occurrences</h3><div class="counter"><a href="#p42">42</a></div><div class="ptext" id="p42">For the top 100 sample data a number of weighted co-occurrences were
                        computed, in essence by multiplying each icon-artifact combination by a
                        factor based on the number of tags assigned to that artifact. Here is an
                        example of two artifacts that rose on the list of calculated similarities as
                        compared to the unweighted data:</div><div id="figure08" class="figure"><div class="ptext"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/figure08.jpg" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/figure08.jpg" alt=""/></a></div><div class="caption"><div class="label">Figure 8. </div> Peter Jenny, <cite class="title italic">"Die minimalen farblichen Abweichungen (...)
                                    werden in der formalen Reduktion zum bildbestimmenden
                                Thema"</cite>. In <cite class="title italic">Bildkonzepte: das wohlgeordnete
                                Durcheinander</cite>. Mainz: Verlag Hermann Schmidt,
                        2000.</div></div><div id="figure09" class="figure"><div class="ptext"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/figure09.jpg" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/figure09.jpg" alt=""/></a></div><div class="caption"><div class="label">Figure 9. </div>Sol Lewitt, <cite class="title italic">Four Basic Colors and all their
                                Combinations</cite>, 1984, indian ink. Private collection (Sol
                            LeWitt) USA. © c/o Pictoright Amsterdam 2008. </div></div><div class="counter"><a href="#p43">43</a></div><div class="ptext" id="p43">The number of icon-artifact associations, <em class="emph">after weighting</em>, was
                        distributed like this: </div><div id="table02" class="table"><table class="table"><tr class="row"><td valign="top" class="cell"/><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i09.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i09.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i10.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i10.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i11.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i11.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i12.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i12.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i13.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i13.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i14.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i14.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i15.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i15.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i16.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i16.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i17.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i17.gif" alt=""/></a></td><td valign="top" class="cell"/></tr><tr class="row"><td valign="top" class="cell">Jenny, <cite class="title italic">Untitled</cite></td><td valign="top" class="cell">15</td><td valign="top" class="cell">2</td><td valign="top" class="cell">4</td><td valign="top" class="cell">2</td><td valign="top" class="cell">2</td><td valign="top" class="cell">0</td><td valign="top" class="cell">1</td><td valign="top" class="cell">1</td><td valign="top" class="cell">1</td><td valign="top" class="cell">28</td></tr><tr class="row"><td valign="top" class="cell">Lewitt, <cite class="title italic">Four Basic Colors</cite></td><td valign="top" class="cell">33</td><td valign="top" class="cell">3</td><td valign="top" class="cell">0</td><td valign="top" class="cell">0</td><td valign="top" class="cell">0</td><td valign="top" class="cell">3</td><td valign="top" class="cell">0</td><td valign="top" class="cell">0</td><td valign="top" class="cell">0</td><td valign="top" class="cell">39</td></tr><tr class="row"><td valign="top" class="cell"/><td valign="top" class="cell">48</td><td valign="top" class="cell">5</td><td valign="top" class="cell">4</td><td valign="top" class="cell">2</td><td valign="top" class="cell">2</td><td valign="top" class="cell">3</td><td valign="top" class="cell">1</td><td valign="top" class="cell">1</td><td valign="top" class="cell">1</td><td valign="top" class="cell">67</td></tr></table><div class="caption-no-label"><div class="label">Table 2. </div></div></div><div class="counter"><a href="#p44">44</a></div><div class="ptext" id="p44">What is most striking in this table is the high frequency of one particular
                        icon shared by both artifacts <em class="emph">and</em> the relatively large number
                        of icons indicating a difference in the images they refer to. The Jenny
                        picture has an unmistakably horizontal articulation, which is lacking in the
                        Lewitt piece. The tagging behavior of subjects in the experiment appears to
                        be visually sound, but our crude computational procedures do not fully
                        reflect that.</div><div class="counter"><a href="#p45">45</a></div><div class="ptext" id="p45">The number of co-occurrences for the Lewitt and Jenny artifacts was 501 (167
                        if the raw data were used). A remarkable shift of this pair of artifacts in
                        the weighted charts was caused by the relative distance between the
                        frequencies of assigned icons: 28 versus 13. Broadly speaking however, in
                        the subset chosen here the differences in approach did rarely result in
                        spectacular shifts. More research is needed to refine the still rudimentary
                        similarity measures.</div></div><div class="div div2"><h3 class="head">One Artifact and its Neighbors</h3><div class="counter"><a href="#p46">46</a></div><div class="ptext" id="p46">Another approach to the inspection of the transaction database is to take one
                        artifact and calculate what in Last.fm terms would be referred to as its
                            <em class="emph">neighbors</em>.<a class="noteRef" href="#d6604e936">[21]</a> We did just that for the
                        Raphael painting presented above (tag frequency: 18). Keeping one value in
                        the comparison constant understandably reduces the effect of weighting. In
                        the top rankings calculated only two pairs of artifacts changed position
                        after weighting.</div><div class="counter"><a href="#p47">47</a></div><div class="ptext" id="p47">Again our sample resulted in rather obvious combinations of artifacts. The
                        look of the Raphael painting is strongly determined by the arch across the
                        full width of the painting, which is reflected in a succession of smaller
                        arches in the axis of the picture plane. More than half of the neighbors
                        displayed such arch-like shapes. Another salient feature of the calculated
                        neighbors is precisely this repetitive quality of shapes containing shapes.
                        Here is an illustrative example of an artifact that co-occurred with the
                        School of Athens:</div><div id="figure10" class="figure"><div class="ptext"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/figure10.jpg" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/figure10.jpg" alt=""/></a></div><div class="caption"><div class="label">Figure 10. </div> Victor Vasarely, <cite class="title italic">Vonal-Ksz.</cite>, 1968
                            vinyl. Private collection. © c/o Pictoright Amsterdam 2008.</div></div><div class="counter"><a href="#p48">48</a></div><div class="ptext" id="p48">This association may at first surprise us, but nevertheless the diagrammatic
                        similarity is apparent. If we look up the corresponding distribution of
                        icon-artifact associations it appears that the high similarity score is
                        completely dependent on the shared icon of two congruent rectangles,
                        connected at the corners by four straight converging lines.</div><div id="table03" class="table"><table class="table"><tr class="row"><td valign="top" class="cell"/><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i02.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i02.gif" alt=""/></a></td><td valign="top" class="cell">…</td><td valign="top" class="cell"/></tr><tr class="row"><td valign="top" class="cell">Raphael, <cite class="title italic">School of Athens</cite></td><td valign="top" class="cell">5</td><td valign="top" class="cell"/><td valign="top" class="cell">18</td></tr><tr class="row"><td valign="top" class="cell">Vasarely, <cite class="title italic">Vonal-Ksz.</cite></td><td valign="top" class="cell">8</td><td valign="top" class="cell">…</td><td valign="top" class="cell">33</td></tr><tr class="row"><td valign="top" class="cell"/><td valign="top" class="cell">13</td><td valign="top" class="cell">…</td><td valign="top" class="cell">51</td></tr></table><div class="caption-no-label"><div class="label">Table 3. </div></div></div><div class="counter"><a href="#p49">49</a></div><div class="ptext" id="p49">Please note that the Vasarely painting has been denoted by a relatively large
                        number of icons <em class="emph">not shared</em> by the Raphael piece. Somehow the
                        icons any two artifacts do <em class="emph">not</em> share should be weighed in a
                        formal definition of similarity. In the next paragraph I will momentarily
                        alter the focus of my analysis and discuss an equation in which both the
                        number and distribution of shared icons and the icons <em class="emph">not</em>
                        shared are considered.</div></div><div class="div div2"><h3 class="head">A Formula Describing a Similarity Measure for Pairs of Artifacts</h3><div class="counter"><a href="#p50">50</a></div><div class="ptext" id="p50">The discussion above was based on an overall analysis of a co-occurrence
                        table and crude distance measures taking the number of shared icons into
                        account. Another approach to similarity measurement is taking any pair of
                        artifacts to compare the corresponding patterns of icon-artifact
                        assignments. If two artifacts share an icon this must add to the overall
                        similarity value. The more icons two artifacts have in common, the more the
                        similarity value should increase. Conversely icons not shared (but used for
                        one of the artifacts in the comparison) should diminish the overall
                        similarity measure. Furthermore the ratio of co-occurrences must also be
                        taken into account. This amounts to weighting the straightforward numbers:
                        if both artifacts in a comparison have an equal number of one particular
                        icon in common, the weight of the corresponding part in the equation could
                        be multiplied by 1 (n:n); if one of the artifacts has been tagged twice as
                        much, the equation could be multiplied by 1/2 (n:2n) and so on. (Of course
                        the numbers must be weighted to optimize results. See below.) So the
                        similarity (ο) might be expressed in the following descriptive formula:
                    </div><div id="formula" class="figure"><div class="ptext"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/formula.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/formula.gif" alt=""/></a></div><div class="caption-no-label"><div class="label">Figure 11. </div></div></div><div class="counter"><a href="#p51">51</a></div><div class="ptext" id="p51"> Here n is the number of shared icons, θ is (per
                        icon shared by both artifacts) the total number of occurrences, s is the minimum and l is the
                        maximum number of occurrences for each icon. The number of icons not shared
                        (per artifact) is m. The number (per icon) of icons not shared by both artifacts is α. N is
                        the sum total of occurrences of icons assigned to both artifacts. As a result of this: if
                        two artifacts do not share any icon, the similarity equals to 0. </div><div class="counter"><a href="#p52">52</a></div><div class="ptext" id="p52">Applying the formula to the (non-normalized) data for the artifacts of
                    figures 6 and 7 this is the outcome: </div><blockquote class="eg"><pre><tt>o = ((15/31*7/8+7/31*2/5+3/31*1/2+2/31*1/1-4/31)+1)/2 = 0.7488</tt></pre></blockquote><div class="counter"><a href="#p53">53</a></div><div class="ptext" id="p53">For the artifacts in figures 8 and 9 <em class="emph">before normalization</em> o
                        would become: </div><blockquote class="eg"><pre><tt>o = ((26/41*11/15+3/41*1/2-12/41)+1)/2 = 0.6045</tt></pre></blockquote><div class="counter"><a href="#p54">54</a></div><div class="ptext" id="p54">Because in the formula s/l is the ratio of the smallest and largest number
                        (per pair of artifacts) the effect of applying the formula to non-normalized
                        data will lead to a pronounced bias. Obviously normalization is a necessary
                        step in preprocessing the raw data if we use the formula.<a class="noteRef" href="#d6604e1024">[22]</a></div></div><div class="div div2"><h3 class="head">Fake Similarities</h3><div class="counter"><a href="#p55">55</a></div><div class="ptext" id="p55">Up to now we have focused on similarity measures that become understandable
                        if we empirically compare the associated icons and artifacts. In the sample
                        data however there are indications of problems caused by the ambiguity of
                        visual tags (icons). Here is an extract of the (normalized) artifact x
                        artifact co-occurrence table, showing only 5 artifacts that were tagged
                        exactly 24 times. </div><div class="table"><table class="table"><tr class="row"><td valign="top" class="cell"/><td valign="top" class="cell">S18500</td><td valign="top" class="cell">S23715</td><td valign="top" class="cell">S75116</td><td valign="top" class="cell">S75122</td><td valign="top" class="cell">S75308</td><td valign="top" class="cell"/></tr><tr class="row"><td valign="top" class="cell">S18500</td><td valign="top" class="cell">137</td><td valign="top" class="cell"/><td valign="top" class="cell"/><td valign="top" class="cell"/><td valign="top" class="cell"/><td valign="top" class="cell">137</td></tr><tr class="row"><td valign="top" class="cell">S23715</td><td valign="top" class="cell"/><td valign="top" class="cell">25</td><td valign="top" class="cell">48</td><td valign="top" class="cell">10</td><td valign="top" class="cell"/><td valign="top" class="cell">83</td></tr><tr class="row"><td valign="top" class="cell">S75116</td><td valign="top" class="cell"/><td valign="top" class="cell"/><td valign="top" class="cell">64</td><td valign="top" class="cell">1</td><td valign="top" class="cell"/><td valign="top" class="cell">65</td></tr><tr class="row"><td valign="top" class="cell">S75122</td><td valign="top" class="cell"/><td valign="top" class="cell"/><td valign="top" class="cell"/><td valign="top" class="cell">41</td><td valign="top" class="cell"/><td valign="top" class="cell">41</td></tr><tr class="row"><td valign="top" class="cell">S75308</td><td valign="top" class="cell"/><td valign="top" class="cell"/><td valign="top" class="cell"/><td valign="top" class="cell"/><td valign="top" class="cell">92</td><td valign="top" class="cell">92</td></tr><tr class="row"><td valign="top" class="cell"/><td valign="top" class="cell"/><td valign="top" class="cell"/><td valign="top" class="cell"/><td valign="top" class="cell"/><td valign="top" class="cell"/><td valign="top" class="cell">418</td></tr></table><div class="caption-no-label"><div class="label">Table 4. </div></div></div><div class="counter"><a href="#p56">56</a></div><div class="ptext" id="p56">In the row and column headings we have the IDs of the five artifacts, while
                        in the table cells there are eight co-occurrence values, five of them being
                        values indicating the number of times an artifact co-occurred with itself.
                        (We will get back to this in the next paragraph.) The three remaining values
                        express co-occurrences of <em class="emph">different</em> artifacts. E.g. S75116 and
                        S23715 refer to waterscape paintings by Claude Monet. The association (n=48)
                        visually makes sense. The other values seem to suggest a similarity between
                        another artifact (S75122) and the two Monet paintings. On closer
                        consideration, however, the icon responsible for the co-occurrence scores
                        appears to have been selected for its bluish appearance in case of the Monet
                        waterscapes, and for its sophisticated color gradients and peculiar
                        configuration of rectilinear lines in case of the other artifact: a painting
                        from the series <cite class="title italic">Homage to the Square:
                        Apparition</cite> (1959), by Josef Albers. In fact the co-occurrence is an
                            "artifact of the method". Here is the icon
                        responsible for the confusion: </div><div class="figure"><div class="ptext"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i18.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i18.gif" alt=""/></a></div><div class="caption-no-label"><div class="label">Figure 12. </div></div></div><div class="counter"><a href="#p57">57</a></div><div class="ptext" id="p57">This must be taken as a warning. Apart from the <em class="emph">vocabulary bias</em>
                        mentioned earlier, ambiguities in the vocabulary in use — our 100 icons —
                        may cause "fake similarities". I will refer to this as
                            <em class="emph">vocabulary ambiguity</em>. (In a section below the inverse of
                        this problem will be discussed, viz. the confusion caused by partly
                        redundant visual tags.)</div></div><div class="div div2"><h3 class="head">Measures Qualifying Individual Artifacts and/or Visual Tags</h3><div class="counter"><a href="#p58">58</a></div><div class="ptext" id="p58">Doubts about the appropriateness of the particular set of icons used in the
                        experiment forced me to take a closer look at the numbers associated with
                        individual artifacts. Again the objective is to verify empirically what
                        numbers in the co-occurrence tables stand for. And again the analysis was
                        done on the top 100 subset in terms of co-occurrences. This time however the
                        values in the pivot table were normalized. Artifacts were ordered according
                        to the total number of co-occurrences, the number of "self
                            co-occurrences" (a multiple of the number of times an artifact
                        was tagged with one or more icons — see above), and the number of
                        co-occurring artifacts (per artifact). Maximum and minimum values, and
                        average, mode and median were also computed. </div><div class="counter"><a href="#p59">59</a></div><div class="ptext" id="p59">Data point out that the number of co-occurrences roughly co-varies with the
                        number of self co-occurrences. The same is true if we first subtract the
                        number of self co-occurrences from the number of co-occurrences. But
                        unfortunately this finding will not bring us near a paradigm shift. </div><div class="counter"><a href="#p60">60</a></div><div class="ptext" id="p60">More interesting was the outcome that the number of self co-occurrences for a
                        particular artifact negatively co-varied with the number of artifacts
                        associated with that artifact (correlation coefficient:
                            -0.6753)<a class="noteRef" href="#d6604e1148">[23]</a>. In other words: the more
                        often an icon (or small set of icons) was assigned to an artifact (in
                        relative terms) the less often that particular artifact could be related to
                        other artifacts. All the same the number of co-occurrences with these few
                        artifacts tended to be high. Another paraphrase of this could be: the
                        smaller the number of partial similarities, the more an artifact
                            "approaches to" one of the icons used. It is clear that
                        this qualifies both artifact and icon (set). The implication of these
                        findings would be that a mere (automatic) analysis of transaction data can
                        be used to improve the set of icons. (See below.) </div><div class="counter"><a href="#p61">61</a></div><div class="ptext" id="p61">If we have a look at the top 10 of artifacts scoring high on "self
                            co-occurrence", we might discern two broad classes of objects:
                        artifacts that are schematic (or: diagrammatic) in appearance, and
                            "rich" (non-schematic) artifacts that have one
                        conspicuous formal feature. Because subjects were asked to "check out the icon that you think matches <em class="emph">any
                                singular</em> outstanding visual characteristic of the piece most
                            of all available icons" the conspicuous traits must have been
                        magnified.</div><div class="counter"><a href="#p62">62</a></div><div class="ptext" id="p62">Given the present set of icons this appeared to be a painting difficult to
                            "describe" unambiguously:</div><div id="figure11" class="figure"><div class="ptext"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/figure11.jpg" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/figure11.jpg" alt=""/></a></div><div class="caption"><div class="label">Figure 13. </div>Paul Gauguin, <cite class="title italic">The Sacred Mountain (Parahi Te
                                Marae)</cite>, 1892, oil on canvas. Philadelphia Museum of Art:
                            Gift of Mr. and Mrs. Rodolphe Meyer de Schauensee, 1980</div></div><div class="counter"><a href="#p63">63</a></div><div class="ptext" id="p63">Several icons seem to have been chosen to indicate the colorfulness of the
                        painting: both the variety of colors and the striking yellow and purple
                        image regions. Others presumably refer to the composition of the picture in
                        three horizontal planes and/or the use of diagonals, the irregular size and
                        shape of color regions. Still others, according to my interpretation, are
                        intended to denote the complementary color contrasts and the
                            "painterliness" of the surface. Anyway it is evident that
                        subjects in the <cite class="title italic">Art.Similarities</cite> experiment
                        were not unanimous in using the visual diagrams offered to tag the Gauguin
                        painting. </div><div class="counter"><a href="#p64">64</a></div><div class="ptext" id="p64">The interpretation of data so far suggests looking at a few other summaries.
                        From the full artifacts x icons table were extracted (per icon) the maximum
                        number of times it was assigned to any artifact. If we divide this maximum
                        value by the grand total for a specific icon, we might have a rough measure
                        of the efficacy of the icon. Values ranged from 0.05 (low efficacy) to 0.5
                        (high efficacy). According to this approach the most and least
                            "problematic" icons in the set (if we skip very low grand
                        totals — say below 25) are these: </div><div class="table"><table class="table"><tr class="row"><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i19.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i19.gif" alt=""/></a></td><td valign="top" class="cell">where n=43; efficacy score=0.05</td></tr><tr class="row"><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i20.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i20.gif" alt=""/></a></td><td valign="top" class="cell">where n=40; efficacy score=0.48</td></tr></table><div class="caption-no-label"><div class="label">Table 5. </div></div></div><div class="counter"><a href="#p65">65</a></div><div class="ptext" id="p65">Of course such numbers should be handled with care: maximum values could be
                        outliers. Perhaps other measures of central tendency (average, median) give
                        better results or could be used to arrive at conclusions that are more firm. </div></div><div class="div div2"><h3 class="head">Falsifications</h3><div class="counter"><a href="#p66">66</a></div><div class="ptext" id="p66">The empirical verification of similarity measures — number of co-occurrences,
                        similarity measure (o) — also made clear that some unmistakably similar
                        artifacts were not recognized as such. Here is a telling example:</div><div id="figure12" class="figure"><div class="ptext"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/figure12.jpg" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/figure12.jpg" alt=""/></a></div><div class="caption"><div class="label">Figure 14. </div>Keith Carter, <cite class="title italic">Silhouette</cite>, 1994,
                            photograph. San Jose Museum of Art: Partial and promised gift of Arthur
                            J. Goodwin. © Keith Carter 2008.</div></div><div id="figure13" class="figure"><div class="ptext"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/figure13.jpg" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/figure13.jpg" alt=""/></a></div><div class="caption"><div class="label">Figure 15. </div>Tracy Boychuk (design), Andrew Eccles (photography), Christopher
                            Austopchuk (art direction), <cite class="title italic">youssou n'dour,
                                    "the guide (wommat)"</cite>, 1994, CD Sleeve
                            design. ©1994 Sony BMG.</div></div><div class="counter"><a href="#p67">67</a></div><div class="ptext" id="p67">The similarity of both images is striking, but they co-occurred only once.
                        Why is that so? If we review the available tags it appears that there is no
                        single icon fit to express the cross-shaped configuration that according to
                        many is the primary similarity feature here. One of the icons seems to be a
                        candidate — and it has indeed been related to the Carter photograph twice;
                        it is a representational black-and-white icon, displaying an umbrella
                        depicted in silhouette:</div><div class="figure"><div class="ptext"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i21.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i21.gif" alt=""/></a></div><div class="caption-no-label"><div class="label">Figure 16. </div></div></div><div class="counter"><a href="#p68">68</a></div><div class="ptext" id="p68">Perhaps the conceptual distance between an umbrella and a human figure
                        disqualified this icon.<a class="noteRef" href="#d6604e1239">[24]</a> The fact that the Carter
                        photograph is a grayscale image, on the other hand, may have diminished this
                        conceptual distance. Other empirical findings seem to support the importance
                        of color in visual tagging. Half of the icons associated with the Boychuk
                        picture were dominantly orange colored; the color palette of eight out of
                        ten was very similar to the palette of the photograph.<a class="noteRef" href="#d6604e1242">[25]</a></div><div class="counter"><a href="#p69">69</a></div><div class="ptext" id="p69">Apart from the (cross-shaped) compositional features that both images have in
                        common, and the dominant color characteristics that — perhaps as a second
                        most conspicuous feature — set them apart, both artifacts apparently share
                        another formal trait, one that — in iconographical terms — may be described
                        as a <em class="emph">silhouette</em>. In the visual vocabulary of <cite class="title italic">Art.Similarities</cite> there are quite a few icons that
                        could be used to denote this image characteristic, but most of these are in
                        black-and-white and show sharp outlines. That could be an explanation for
                        the missing match in this case. Both images show vague outlines (conspicuous
                        similarity feature number four). In fact there is one icon with the
                        representation of a human figure, which could be qualified as a silhouette,
                        and it was used to tag the Boychuk artifact. But it has an orangish
                        background, which apparently disqualifies it for the Carter photograph.</div><div class="counter"><a href="#p70">70</a></div><div class="ptext" id="p70">My rather lengthy discussion of this one falsification casus is intended to
                        introduce a few speculations about the promises of using visual tags. It
                        makes clear that successful tagging is dependent on a balanced set of visual
                        tags (icons). The size and the number of these visual tags are clearly
                        decisive for effective tagging. Furthermore the following principles should
                        be taken into account: </div><div class="ptext"><ul class="list"><li class="item">Color phenomena may have a strong impact on efficient tagging. </li><li class="item">The presence of basic compositional features in the tag set is
                            decisive. </li><li class="item">Representational elements may interfere with the tagging of formal
                            artifact characteristics. </li></ul></div><div class="counter"><a href="#p71">71</a></div><div class="ptext" id="p71">And finally: </div><div class="ptext"><ul class="list"><li class="item">Refined overall local features, as in the qualification
                                "vagueness" (of contour), will be hard to express
                            unambiguously.</li></ul></div></div></div><div class="div div1"><h2 class="head">Observer Behavior</h2><div class="counter"><a href="#p72">72</a></div><div class="ptext" id="p72">Even in the simple prototype application discussed here, it is possible to
                    retrieve all the image-icon assignments of one particular person. It is
                    fascinating to inspect the recorded visual discernments of individuals, because
                    they seem to give away observer habits. A sound follow-up procedure would be to
                    compare central tendencies in the discernments of several observers. The
                    outcomes can be stored in the database as <em class="emph">performance data</em>. If two
                    observers have made similar image-icon associations, then possibly they resemble
                    one another in the way their mental processes organize the visual world. What if
                    one of these individuals is recognized as an authority? Would that somehow
                    qualify the other? And if two qualified art historians agree on what is typical
                    of a series of artifacts, would that be the beginning of "proof"?</div><div class="counter"><a href="#p73">73</a></div><div class="ptext" id="p73">I made the point earlier, and I shall make it again, that the random presentation
                    of artifacts in this experiment must have affected similarity scores. Another
                    restriction is that subjects (n=204) were free to tag images. The top tagger
                    assigned icons 141 times and 19 subjects only tagged once. Obviously absolute
                    pronouncements based on these figures cannot be made. This is all the more true
                    where as to the dimension of subject characteristics — prior knowledge,
                    professional merits, age, race, etc. — an empirical verification of analysis
                    outcomes would have implied a more extensive survey. This was beyond the
                    ambitions of the current experiment. </div><div class="counter"><a href="#p74">74</a></div><div class="ptext" id="p74">So the dataset in this experiment is too restricted to justify a thorough
                    analysis of the behavior of subjects. With 400 artifacts and 100 icons we have
                    40.000 possible combinations. In the database only 2242 combinations occurred on
                    a total of 3918 records, giving an average of less than 2 hits per combination.
                    It is clear that few co-occurrence scores can be computed from these numbers.
                    After the grouping of observers by artifact-icon combinations I calculated 5317
                    co-occurrences for all subjects, the highest number being 15. In principle such
                    numbers can be used to compute similar observers, but given the restricted data
                    I will not elaborate on these data now.</div><div class="counter"><a href="#p75">75</a></div><div class="ptext" id="p75">Inspecting the voter x voter pivot table I noticed one peculiar phenomenon: the
                    rows and columns of some subjects displayed a striking low number of
                    co-occurrence scores. This appeared to be independent of the number of tags
                    assigned by these subjects. What could be the cause of these outliers? One
                    explanation is that the outliers correspond to subjects noticing visual
                    subtleties that remain hidden for other observers. Another, I think more
                    plausible explanation is that the outliers were caused by
                    "clumsy" individuals, unable to make up their mind about the most
                        <em class="emph">outstanding visual characteristics</em> of the artifacts on display,
                    or even worse maybe, by subjects unable to understand the task at hand. </div><div class="counter"><a href="#p76">76</a></div><div class="ptext" id="p76">One might expect a correlation between the number of co-occurrences and the
                    number of unique icon-artifact assignments. For a sample of 10 subjects in the
                    middle range of tag frequencies (n=41–63) I computed for each individual in the
                    transaction table the ratio of the number of tags assigned and the number of
                    unique icon-artifact assignments. For obvious reasons there was a strong
                    correlation with the ratio of the sum of all co-occurrence values and the number
                    of co-occurrence values (r=0.94). </div><div class="counter"><a href="#p77">77</a></div><div class="ptext" id="p77">A quick-and-dirty reality check pointed out that my outliers were caused by
                    students with rather feeble study results, whereas — conversely — a highly
                    successful student was responsible for the top score. Especially the latter
                    finding seems to suggest that a good student in art history is able to discern
                    visual features that are in conformity with the discernments of his or her
                    peers. Of course we must be very careful with generalizations of this kind. It
                    would be more profitable to search for correlations related to an individual's
                    preferences on dimensions like: time period, geographical location, or even
                    iconographical subjects.</div></div><div class="div div1"><h2 class="head">Icon Covariance</h2><div class="counter"><a href="#p78">78</a></div><div class="ptext" id="p78">In my experiment the selection of icons was non-arbitrary.<a class="noteRef" href="#d6604e1302">[26]</a> In fact
                    a priori visual knowledge was used to compile an effective (i.e. varied) set of
                    icons, starting from a base collection of about 500 diagrams. There is a
                    drawback to this non-evolutionary approach. The process may become biased. I may
                    have unknowingly excluded options for expressing specific visual concepts. And
                    indeed, the discussion of falsifications in the previous section gave an example
                    of such a probable omission. I will now consider the opposite phenomenon, viz.
                        <em class="emph">vocabulary redundancy</em>, which is here defined as a lack of
                    efficacy in the icon set due to the existence of icons that are apparently used
                    to denote same or almost same visual concepts.<a class="noteRef" href="#d6604e1314">[27]</a></div><div class="counter"><a href="#p79">79</a></div><div class="ptext" id="p79">The assumption is that icons appropriate to refer to a specific visual feature
                    will co-occur more often than may be expected in the case of purely disjointed
                    icons. Of course there is the risk that the set of artifacts used in this
                    experiment displays a few "real world" conjunctions — as a matter
                    of fact I have an example of that below. Therefore I repeat that my data
                    analysis can only be indicative. Nevertheless I think that from the results of
                    the present experiment some generalizations can be made. </div><div class="counter"><a href="#p80">80</a></div><div class="ptext" id="p80">This time in the original transaction table I grouped the icons that had been
                    assigned to each individual artifact. For the resulting subsets of icons
                    co-occurrences were computed. The sum of all possible pairs is (100<span class="hi superscript">2</span>+100)/2=5050, since we have 100 icons and since
                    co-occurrences of same icons are relevant as well. In the transaction database
                    3137 unique combinations occurred. Only 7 icons were never assigned more than
                    once to a particular artifact.<a class="noteRef" href="#d6604e1331">[28]</a> The sum
                    total of icon pairs in our database was 24.826. So the average number of
                    co-occurrences — if we skip the zero-values — is about 8. Here is the top 5 of
                    pairs of icons in terms of co-occurrences: </div><div class="table"><table class="table"><tr class="row"><td valign="top" class="cell">Icon pairs</td><td valign="top" class="cell">Co-occurrences</td></tr><tr class="row"><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i02.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i02.gif" alt=""/></a> and <a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i12.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i12.gif" alt=""/></a></td><td valign="top" class="cell">204</td></tr><tr class="row"><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i22.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i22.gif" alt=""/></a> and <a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i23.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i23.gif" alt=""/></a></td><td valign="top" class="cell">136</td></tr><tr class="row"><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i24.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i24.gif" alt=""/></a> and <a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i25.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i25.gif" alt=""/></a></td><td valign="top" class="cell">119</td></tr><tr class="row"><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i22.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i22.gif" alt=""/></a> and <a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i26.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i26.gif" alt=""/></a></td><td valign="top" class="cell">116</td></tr><tr class="row"><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i25.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i25.gif" alt=""/></a> and <a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i27.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i27.gif" alt=""/></a></td><td valign="top" class="cell">114</td></tr></table><div class="caption-no-label"><div class="label">Table 6. </div></div></div><div class="counter"><a href="#p81">81</a></div><div class="ptext" id="p81">This way of visualizing co-occurrences may create the impression that
                    redundancies in our vocabulary indeed exist. The fact that mere numbers seem to
                    indicate an overlap in the visual features of a set of schematic images is
                    captivating. None of our subjects ever alleged this! So could the manipulation
                    of visual things by individuals amount to some sort of collaborative visual
                    knowledge, a knowledge that emerges from our transaction database? </div><div class="counter"><a href="#p82">82</a></div><div class="ptext" id="p82">Just to check on this I zoomed in on co-occurrences for the pair of icons for
                    which n=114. Here are the results: </div><div class="table"><table class="table"><tr class="row"><td valign="top" class="cell"/><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i24.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i24.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i27.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i27.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i28.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i28.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i29.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i29.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i13.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i13.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i30.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i30.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i02.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i02.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i31.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i31.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i10.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i10.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i32.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i32.gif" alt=""/></a></td></tr><tr class="row"><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i25.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i25.gif" alt=""/></a></td><td valign="top" class="cell">119</td><td valign="top" class="cell">114</td><td valign="top" class="cell">89</td><td valign="top" class="cell">46</td><td valign="top" class="cell">29</td><td valign="top" class="cell">26</td><td valign="top" class="cell">20</td><td valign="top" class="cell">19</td><td valign="top" class="cell">18</td><td valign="top" class="cell">16</td></tr></table><div class="caption-no-label"><div class="label">Table 7. </div></div></div><div class="table"><table class="table"><tr class="row"><td valign="top" class="cell"/><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i25.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i25.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i24.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i24.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i29.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i29.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i12.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i12.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i13.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i13.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i32.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i32.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i33.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i33.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i30.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i30.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i34.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i34.gif" alt=""/></a></td><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i31.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i31.gif" alt=""/></a></td></tr><tr class="row"><td valign="top" class="cell"><a href="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i27.gif" rel="external"><img src="http://www.digitalhumanities.org/dhq/vol/2/1/000019/resources/images/i27.gif" alt=""/></a></td><td valign="top" class="cell">114</td><td valign="top" class="cell">84</td><td valign="top" class="cell">29</td><td valign="top" class="cell">23</td><td valign="top" class="cell">21</td><td valign="top" class="cell">17</td><td valign="top" class="cell">13</td><td valign="top" class="cell">13</td><td valign="top" class="cell">13</td><td valign="top" class="cell">12</td></tr></table><div class="caption-no-label"><div class="label">Table 8. </div></div></div><div class="counter"><a href="#p83">83</a></div><div class="ptext" id="p83">In this particular experiment we must be cautious: the relatively small number of
                    objects in the database could affect the number of co-occurrences computed. As a
                    matter of fact there are two possible causes of high frequencies for
                    co-occurrences: </div><div class="ptext"><ol class="list"><li class="item">Icons in the tag set are visually similar. (I would suggest they are
                        redundant.)</li><li class="item">A particular combination of visual characteristics is prevailing in the
                        artifacts considered.</li></ol></div><div class="counter"><a href="#p84">84</a></div><div class="ptext" id="p84">It is tempting to forget about the reality these icons refer to, and to read each
                    table as a summary of the alternative interpretations of the icons on the left.
                    On the other hand, the more the co-occurring icons classify the icon on the
                    left, the more redundancy we presumably have in the set. Furthermore I expect
                    that the similarity between the icon being analyzed and the co-occurring icons
                    diminishes from left to right. In the table just above the first icon in the row
                    — the one with the spokes — is ostensibly similar to the icon of our focus. The
                    appearance of a green-and-blue icon however is less obvious in an immediate
                    comparison. An empirical check gives away that the co-occurrence value here is
                    caused by pictures of landscapes with an easily tractable vanishing point and
                    guidelines.</div><div class="counter"><a href="#p85">85</a></div><div class="ptext" id="p85">Unfortunately I have no data at hand to support this line of thought. Still I
                    think it is safe to say that the mere number of co-occurrences with other icons
                    in the tagging set suggests that the icons are partly redundant indeed.<a class="noteRef" href="#d6604e1489">[29]</a> Knowledge of this kind can be used to
                    improve the (initial) set of icons. What than could be a rule for detecting
                    redundancies in the set of icons? I think such a rule should both consider the
                    number of co-occurrences and the number and distribution of other icons in the
                    neighborhood of the icons compared, e.g.: </div><div class="counter"><a href="#p86">86</a></div><div class="ptext" id="p86"><em class="emph">If two icons are in the top n of co-occurrences, and the top n of
                        co-occurrences for these icons is composed of the same icons, than the icons
                        are at least partly redundant and a "merge operation" to get
                        rid of the redundancy is justified.</em></div><div class="counter"><a href="#p87">87</a></div><div class="ptext" id="p87">The challenge for future research is to make such rules for an automated analysis
                    explicit.</div></div></div><div class="div div0"><h1 class="head">Conclusions</h1><div class="counter"><a href="#p88">88</a></div><div class="ptext" id="p88">As was contended in the Introduction, advocates of humanities computing stressed the
                fact that computational techniques may have the collateral benefit of indicating
                where in the domain of scholarly research "there are holes in your sketch, [and] where it breaks down" [<a class="ref" href="#unsworth2001">Unsworth 2001</a>]. Add to this that in the humanities the process of fine tuning personal
                interests and cognitive schemata is paramount. There are few if any final answers as
                to meaning in the visual arts: "the process is often at least as important as the product,
                        and the process itself produces new knowledge and understanding" [<a class="ref" href="#jorgensen1999">Jörgensen 1999</a>, 315]. That is exactly why I have good hopes that applications like <cite class="title italic">Art.Similarities</cite> will be beneficial in art/art history
                education. More in general I think that the experiment has demonstrated that Web 2.0
                technologies can offer new opportunities in humanities research.</div><div class="counter"><a href="#p89">89</a></div><div class="ptext" id="p89">From the analysis section we learned: <ul class="list"><li class="item">that meaningful perspectives on visual artifacts can be pointed out by
                        visual means; </li><li class="item">that useful similarity measures can be derived from multiple (simple)
                        similarity expressions;</li><li class="item">that the composition of efficient visual vocabularies may benefit from
                        visual tagging.</li></ul></div><div class="counter"><a href="#p90">90</a></div><div class="ptext" id="p90">Stock-taking here was primarily aimed at indicating the essential possibilities of a
                visual labeling and harvesting paradigm for art historically relevant visual traits.
                But our field test also indicated some limitations of visual labeling. Color is
                problematic; the size of our displays units is (still) limiting; the power to
                differentiate between stylistically similar works of art is still indeterminate; we
                have no sharp idea of what an ideal set of visual denotats may look like; we don't
                know in what degree such a set could be universal. Nevertheless <cite class="title italic">Art.Similarities</cite> also suggested possible directions for future
                research.</div><div class="counter"><a href="#p91">91</a></div><div class="ptext" id="p91">One drawback of collaborative filtering systems is their tendency not to cover the
                extreme cases [<a class="ref" href="#chen2005">Chen &amp; McCleod 2005</a>], [<a class="ref" href="#sierra2007">Sierra 2007</a>]. This issue
                also came up with the visual voting system proposed here. How can we direct the
                system to encode the significance of valuable exceptions to the rule, and how can we
                make it ignore cheating, trivialities and clichés? Some data mining protocol for
                judging automatically traced deviant votes (outliers) and blowing up or playing down
                their relative importance would be a valuable extension to <cite class="title italic">Art.Similarities</cite>.<a class="noteRef" href="#d6604e1544">[30]</a></div><div class="counter"><a href="#p92">92</a></div><div class="ptext" id="p92">Another extension would be the coordination of different solutions to the retrieval
                of information about same object types. This paper offered rough sketches of a
                method of classifying visual artifacts independently of other classifying options.
                But the interdependency of different classification systems is worth considering.
                Iconographical descriptions ("aboutness") and descriptions of visual
                qualities ("offness") belong to separate notational spheres [<a class="ref" href="#panofsky1970">Panofsky 1970</a>], [<a class="ref" href="#batschmann1986">Bätschmann 1986</a>], [<a class="ref" href="#nauta1993">Nauta 1993</a>], [<a class="ref" href="#shatfordlayne2002">Shatford Layne 2002</a>]. Still there is an
                apparent covariance of these description options: if two artifacts are depictions of
                a particular theme, chances are that both artifacts share a considerable number of
                visual characteristics. Such partial description parallels also exist for other
                types of metadata. For example: once an object has been classified as an
                    "engraving" (qua technique), it is usually redundant to file the
                artifact as a "grayscale" picture, or as a picture "having
                    linear qualities". So we could fine tune the current approach by
                introducing restrictions: if images are in grayscale, don't offer colored icons, and
                vice versa. Eventually this may lead to the use of exchangeable icon sets, and the
                related question of how to select these sets of visual icons.</div><div class="counter"><a href="#p93">93</a></div><div class="ptext" id="p93">Conventional scholarly communication in art history amounts to a textual exchange of
                information in perhaps 95% of the cases. With that in mind the idea might come up
                that somehow the results of visual voting actions should be translated back into
                textual arguments. Now how will we ever get back to words? Since today's information
                technology is truly interactive in nature, conclusions based on coordinated visual
                discernments can be immediately communicated back to the communities involved in the
                process. One group of observers could be involved in visual voting, while the
                results from this "subroutine" may be fed back to another part — say
                the "Next Level" users — of the population. If the second group uses
                some sort of verbal labeling interface, like the one developed by the Steve.museum
                initiative [<a class="ref" href="#wyman2006">Wyman et al. 2006</a>], promising cross-fertilizations become
                possible. With the adoption of appropriate standards, an integration of textual
                annotations, reviews, or the discussion of remarkable patterns in visual behavior
                becomes possible. In a way the collaborative visual filtering system enables
                noticing, labeling and re-labeling phenomena, thus giving shape to an augmented
                social construction of knowledge [<a class="ref" href="#barrett1989">Barrett 1989</a>]. The role of a scholar
                in designing these filtering systems will not be the role of an author of scholarly
                articles or monographs, but the role of a broker in art historical knowledge, the
                director of an intricate process of knowledge mediation [<a class="ref" href="#stubenrauch1993">Stubenrauch 1993</a>]. </div><div class="counter"><a href="#p94">94</a></div><div class="ptext" id="p94">If it is assumed that there is no reason to strive for a one-fits-all collection of
                visual denotats, extended research might be focused on the matching of specific
                collections of visual artifacts and appropriate sets of icons. Such research efforts
                could start from the assumption that when the medium of expression is fixed,
                observers (experts and/or lay people) will just express themselves in the reference
                materials at hand, focusing in these denotats on what are their peculiar points for
                attention. This actually simplifies research, since developers need not be explicit
                about suitable reference options. A myriad of further extensions to the basic visual
                voting model is now conceivable. Observers from particular user groups, for
                instance, might be enabled to work with sets of dedicated icons ("private alphabets"). We predict that as a matter of course
                these series will be composed according to common sense principles, like maximum
                diversity, sufficient coverage, etc.</div><div class="counter"><a href="#p95">95</a></div><div class="ptext" id="p95">Extensions of the voting system may include additional functionalities. One might be
                the possibility to define subsets of artifacts, based on singular or multiple
                corresponding artifact-icon associations; these subsets would thus be true
                collaborative products. Based on an analysis of patterns in voting behavior (or,
                alternatively, explicit observer profiles), the system could also gradually evolve
                into a system displaying only the preferences (both artifacts and/or votes) of a
                particular user group. Finally, since somehow artifacts co-organize icons, the
                labels (icons) in the interface may be grouped (i.e. organized) by their
                co-occurence patterns; perhaps these may be taken as so-called <em class="emph">emergent
                    semantics</em>[<a class="ref" href="#aberer2004">Aberer et al. 2004</a>]. </div><div class="counter"><a href="#p96">96</a></div><div class="ptext" id="p96">Hopefully this orientation has made clear that the social formation of opinions
                concerning the visual similarity of cultural artifacts amongst a group of motivated
                observers <em class="emph">can</em> with the use of information technology be modeled and
                recorded <em class="emph">without</em> the application of verbal descriptors, and that
                interesting data processing options exist for collaboratively and visually
                characterized artifacts. This offers many starting points for further research.</div></div></div><div id="notes"><h2>Notes</h2><div class="endnote" id="d6604e85"><span class="noteRef">[1]</span>Other interesting examples: <a class="ref" href="http://www.connotea.org/" onclick="window.open('http://www.connotea.org/'); return false">Connotea</a>, <a class="ref" href="http://www.librarything.com/" onclick="window.open('http://www.librarything.com/'); return false">LibraryThing</a>, <a class="ref" href="http://movielens.org/" onclick="window.open('http://movielens.org/'); return false">MovieLens</a>, <a class="ref" href="http://www.last.fm/" onclick="window.open('http://www.last.fm/'); return false">Last.fm</a>, <a class="ref" href="http://www.steve.museum/" onclick="window.open('http://www.steve.museum/'); return false">Steve.museum</a>. The list is
                open-ended.</div><div class="endnote" id="d6604e103"><span class="noteRef">[2]</span>"Collaborative filtering is the method of making
                                automatic predictions (filtering) about the interests of a user by
                                collecting taste information from many users (collaborating)" (<a class="ref" href="http://en.wikipedia.org/wiki/collaborative_filtering" onclick="window.open('http://en.wikipedia.org/wiki/collaborative_filtering'); return false">http://en.wikipedia.org/wiki/Collaborative_filtering</a>).
                    For more extensive reading, refer to [<a class="ref" href="#bra2004">Bra &amp; Neijdl 2004</a>], [<a class="ref" href="#heylighen1999">Heylighen 1999</a>] and [<a class="ref" href="#lynch2001">Lynch 2001</a>].
                </div><div class="endnote" id="d6604e117"><span class="noteRef">[3]</span> A question indicative of ever shifting intellectual property
                    concerns is whether surreptitious measurements of user behavior should be
                    considered a breach of copyright laws. That someone's purchasing behavior is a
                    creative act may be defendable.</div><div class="endnote" id="d6604e122"><span class="noteRef">[4]</span>The value of such an exercise could be in testing the methods of the field
                        under consideration. Advocates of humanities computing like John Unsworth
                        have stressed the fact that computational techniques may have the collateral
                        benefit of indicating where are methodological flaws in scholarly research: <blockquote><p>"The implementation of a spec in
                                    a program appears as a kind of critique of the text of the
                                spec" — which is to say, when you sketch what you think you
                                understand, and then try to turn that sketch into a series of
                                instructions a computer can execute, you find out where there are
                                holes in your sketch, where it breaks down. Similarly, when you
                                design an information structure and then try to fit the actual
                                information into it, you find out where the structure doesn’t fit.
                                That’s the back and forth between part and whole, rule and instance. [<a class="ref" href="#unsworth2001">Unsworth 2001</a>]</p></blockquote><br/><br/></div><div class="endnote" id="d6604e136"><span class="noteRef">[5]</span> For a comprehensive project on collaborative
                        <em class="emph">verbal</em> tagging (in the context of museum information systems),
                    refer to the <a class="ref" href="http://www.steve.museum/" onclick="window.open('http://www.steve.museum/'); return false">Steve.museum</a> project.
                    Cf. [<a class="ref" href="#bearman2005">Bearman &amp; Trant 2005</a>] and [<a class="ref" href="#wyman2006">Wyman et al. 2006</a>]. See also [<a class="ref" href="#ross2004">Ross et al. 2004</a>, 145–179] for contextual materials. A more general
                    verbal tagging application, much discussed, is <cite class="title italic">Google Image
                        Labeler</cite>, based on the work of Luis von Ahn. Cf. [<a class="ref" href="#oreilly2006">O'Reilly 2006</a>].</div><div class="endnote" id="d6604e201"><span class="noteRef">[6]</span>
                    For a classic study refer to [<a class="ref" href="#wolfflin1950">Wölfflin 1950</a>].</div><div class="endnote" id="d6604e208"><span class="noteRef">[7]</span>Here the word "effective" is used in the sense of "being able
                        to get at the record"[<a class="ref" href="#bush1945">Bush 1945</a>], which is a modest objective. One of the rare attempts
                    to communicate in a systematic way about certain (compositional) features of
                    works of art is Erle Loran's controversial book on Cézanne's composition [<a class="ref" href="#loran2006">Loran 2006</a>]. Loran developed a diagrammatic language to point at
                    organizational principles in the paintings of Cézanne. His diagrams, however,
                    are fairly dependent on the oeuvre being studied. For the present experiment
                    they are not sufficiently simple. It is my aim to use more
                    "attenuated" diagrammatic images in order to communicate less
                    specific image characteristics.</div><div class="endnote" id="d6604e286"><span class="noteRef">[8]</span>In a sense this is a loose generalization of what as early
                    as 1950 was concluded by Fred Attneave after a series of experiments using
                    simple visual stimuli [<a class="ref" href="#attneave1950">Attneave 1950</a>]. Attneave's findings
                    initiated a prolific line of research in the domain of cognitive psychology and
                    computer vision.</div><div class="endnote" id="d6604e296"><span class="noteRef">[9]</span>Some support for
                    this assumption I found in Jörgensen [<a class="ref" href="#jorgensen1999">Jörgensen 1999</a>, 305].</div><div class="endnote" id="d6604e318"><span class="noteRef">[10]</span>Russel A. Kirsch defined these as "intermediate sources"[<a class="ref" href="#kirsch1984">Kirsch 1984</a>].</div><div class="endnote" id="d6604e325"><span class="noteRef">[11]</span>A visual <em class="emph">denotat</em> is defined here as an actual
                    feature or formal characteristic of a visual artifact, which an observer regards
                    as meaningful and communicable, and which may be referred to by a visual
                    abstraction of that feature. It may be contrasted with the
                    <em class="emph">connotat</em>, which is any emotional or otherwise associative response
                    to an artifact which, though meaningful privately, cannot be meaningfully
                    communicated in abstractable visual form.</div><div class="endnote" id="d6604e340"><span class="noteRef">[12]</span>Consider this as a special type of visual similarity, where
                    one image is <em class="emph">replete</em> and the other <em class="emph">simple </em>(or, as
                    Goodman worded it, <em class="emph">attenuated</em>). The relationship is
                        <em class="emph">unbalanced. </em>The simple image might be said to
                    <em class="emph">clarify</em> one trait in the replete image, acting this way as a visual
                    denominator. Cf. [<a class="ref" href="#goodman1988">Goodman 1988</a>]. See <a href="#note14">note
                    14</a> in this article.</div><div class="endnote" id="d6604e364"><span class="noteRef">[13]</span>It might be objected that even a simple (iconic) image may be (visually) ambiguous, up to the point where the nature of its
                    similarity to a replete visual artifact becomes obscure. This objection is
                    refutable by proposing that whenever associations are ambiguous it is the
                    presentation <em class="emph">context</em> that will determine the probability of
                    specific readings. Compare this to the ambiguity of a word like
                    "Bank", where "Bank" + "of England"
                    &lt;&gt; "Bank" + "under water".</div><div class="endnote" id="d6604e454"><span class="noteRef">[14]</span>"Replete" is the word used by Nelson
                        Goodman to indicate that (in certain images, particularly works of art)
                        every namable trait is or may be decisive for the overall aesthetic effect.
                        The opposite of "replete" in Goodman's vocabulary
                        is "attenuated"[<a class="ref" href="#goodman1988">Goodman 1988</a>, 230n]. The concept is used here to
                        distinguish full, rich, multi-trait artifacts from simple (attenuated)
                        icons. See also [<a class="ref" href="#elkins1999">Elkins 1999</a>, 70].</div><div class="endnote" id="d6604e520"><span class="noteRef">[15]</span>Subsequent research may lead to
                        systems where another type of object is involved, viz. the <em class="emph">textual
                            argumentation</em> of individuals, worded ad hoc or in the manner of
                        familiar ontologies.</div><div class="endnote" id="d6604e538"><span class="noteRef">[16]</span>In future systems additional attributes can
                            be added in separate tables to record various kinds of <em class="emph">performance
                                data</em>.</div><div class="endnote" id="d6604e548"><span class="noteRef">[17]</span>Here are two examples of additional attributes of votes
                            that might or even should be incorporated in future versions of the
                            system. First, normalized picture coordinates: these might be stored to
                            record regions of interest in cases where outstanding features are
                            distinctly localized. And second, the kind of task involved: asking an
                            observer to indicate what is the most salient image characteristic will
                            lead to a very different significance of voting behavior results as
                            compared to asking for giving votes to some trivial or even any image
                            feature.</div><div class="endnote" id="d6604e561"><span class="noteRef">[18]</span>An exhaustive discussion of data dictionaries falls outside the
                        scope of this paper.</div><div class="endnote" id="d6604e608"><span class="noteRef">[19]</span> The inclusion of choice pairs of replete images
                        sharing some apparent visual configuration was considered, e.g. two highly
                        symmetrical images, two images pronounced to be similar according to a
                        widely respected art historian, or even two copies of the same work. The
                        only "testers" actually included are a pair of almost
                        identical artifact reproductions — a painting by Seurat (<cite class="title italic">Bathers at Asnières</cite>, 1883) and a preparatory
                        sketch of the same piece (<cite class="title italic">Final Study for "Bathers at Asnières"</cite>, 1883), and two
                        similar images from a textbook on pictorial concepts <span class="error"><a class="ref" href="#jenny2000">#jenny2000</a></span>. It is a quick-and-dirty setup to see
                        if these image pairs would be indexed in a predictable manner. They
                    were.</div><div class="endnote" id="d6604e678"><span class="noteRef">[20]</span>"Eye-catching", to put it plainly, implies a response of
                            contemporary observers. The historicity of my approach will be discussed
                            in a separate publication.</div><div class="endnote" id="d6604e936"><span class="noteRef">[21]</span>Cf. <a class="ref" href="http://www.last.fm/help/faq/" onclick="window.open('http://www.last.fm/help/faq/'); return false">http://www.last.fm/help/faq/</a>. </div><div class="endnote" id="d6604e1024"><span class="noteRef">[22]</span>Following
                            this adaption the similarity measure for figures 6 and 7 would become:
                            0.7319 (a slight decrease in value). For figures 8 and 9 o after
                            normalization would be: 0.5832 (also a slight decrease).</div><div class="endnote" id="d6604e1148"><span class="noteRef">[23]</span>Compare this to the correlation coefficient for the sum of
                                "non-self" co-occurrences and the number of
                            associated artifacts, which is -0.1504.</div><div class="endnote" id="d6604e1239"><span class="noteRef">[24]</span>Time and again I found that as soon as pictures
                            could be perceived as depictions, the objective of tagging formal image
                            characteristics was not fulfilled.</div><div class="endnote" id="d6604e1242"><span class="noteRef">[25]</span>I compared
                            histogram values of the Boychuk picture and all of the associated icons
                            taken together: mean and median for the hue channel were exactly the
                            same: mean=23, median=22.</div><div class="endnote" id="d6604e1302"><span class="noteRef">[26]</span>Starting from the
                        premise that the <em class="emph">actual</em> manipulation of indexing terms would
                        take a meaningful form, reflecting visual habits/knowledge in the observer
                        group, the initial plan was to supply the community with a set of
                            <em class="emph">random</em> visual signs. From an analysis of the icon use logs,
                        after a substantial amount of time, would emerge traces of visual thinking.
                        The whole process might be made evolutionary. Some icons might be used too
                        often, reducing informational value; some might never be used at all; some
                        icons might tend to appear together in most of the cases. There might be
                        pairs of icons co-appearing with another icon, where the latter one could be
                        taken as a merge (component) icon of both the others, and so on. The initial
                        set of icons might be restricted or extended based upon results from the
                        analysis. This way the effectiveness of the visual vocabulary in use would
                        be improved, while the relevant visual notions could surface.</div><div class="endnote" id="d6604e1314"><span class="noteRef">[27]</span><em class="emph">Vocabulary bias</em>
                        and <em class="emph">vocabulary ambiguity</em> were considered in an earlier section
                        of this paper.</div><div class="endnote" id="d6604e1331"><span class="noteRef">[28]</span> Notwithstanding the fact that our sample is
                        modest, I assume that a relatively low number of self co-occurrences may
                        also count as a crude measure of icon efficacy. Compare this to the earlier
                        paragraph on icon efficacy. If we take the ratio of the number of times an
                        icon co-occurred with itself and the frequency of that same icon as a
                        measure of efficacy, the icons in our earlier example become 0.16 (for the
                            "golden glow" icon) and 4.45 (for the "low
                        horizon" icon) on a scale ranging from 0.07 to 6.86.</div><div class="endnote" id="d6604e1489"><span class="noteRef">[29]</span>The
                            <em class="emph">differences</em> in both sets of co-occurring icons on the other
                        hand are informative as well. The vortex-like shape having a high
                        co-occurrence value in the upper table does not show up in the lower table,
                        and in the lower table we have a checker board which would be visually
                        unsound in the upper table. These differences are immediate qualifications
                        of the icons being compared. </div><div class="endnote" id="d6604e1544"><span class="noteRef">[30]</span> For <cite class="title italic">Art.Similarities</cite> work in progress is focused on "detecting Level
                        2" observers and approval recording. Refer to steps 16–18 from the
                    experimental setup. See also Luis von Ahn's "games with a
                        purpose"[<a class="ref" href="#ahn2006">Ahn 2006</a>].</div></div><div id="worksCited"><h2>Works Cited</h2><div class="bibl fallback"><span class="ref" id="aberer2004"><!-- close -->Aberer et al. 2004</span> Aberer, Karl, Philippe Cudré-Maroux, and Aris
                M. Ouksel, eds. "Emergent Semantics Principles and
                Issues."IFIP 2.6 Working Group on Data Semantics: Research Group of Distributed
                Information Systems (SID), University of Zaragoza, 2004.
            Online version at <a class="ref" href="http://sid.cps.unizar.es/publications/postscripts/dasfaa04.pdf" onclick="window.open('http://sid.cps.unizar.es/publications/postscripts/dasfaa04.pdf'); return false">http://sid.cps.unizar.es/PUBLICATIONS/POSTSCRIPTS/dasfaa04.pdf</a>.</div><div class="bibl fallback"><span class="ref" id="ahn2006"><!-- close -->Ahn 2006</span> Ahn, Luis von. "Games With a Purpose."<cite class="title italic">Computer</cite>39.6 (2006): 92–94. Online version at <a class="ref" href="http://www.cs.cmu.edu/~biglou/ieee-gwap.pdf" onclick="window.open('http://www.cs.cmu.edu/~biglou/ieee-gwap.pdf'); return false">http://www.cs.cmu.edu/~biglou/ieee-gwap.pdf</a>.</div><div class="bibl fallback"><span class="ref" id="attneave1950"><!-- close -->Attneave 1950</span> Attneave, Fred. "Dimensions of similarity."<cite class="title italic">American Journal of Psychology</cite>63.4 (1950): 516–556. </div><div class="bibl fallback"><span class="ref" id="baca2005"><!-- close -->Baca &amp; Harpring 2005</span> Baca, Murtha, and Patricia Harpring. <cite class="title italic">Categories for the Description of Works of Art (CDWA)</cite>.
                Los Angeles, CA: J. Paul Getty Trust and College Art
                Association, Inc., 2005. Online version at <a class="ref" href="http://www.getty.edu/research/conducting_research/standards/cdwa/" onclick="window.open('http://www.getty.edu/research/conducting_research/standards/cdwa/'); return false">http://www.getty.edu/research/conducting_research/standards/cdwa/</a>.</div><div class="bibl fallback"><span class="ref" id="barrett1989"><!-- close -->Barrett 1989</span> Barrett, Edward. "Introduction: Thought and
                Language in a Virtual Environment." In <cite class="title italic">The Society of
                Text: Hypertext, Hypermedia, and the Social Construction of Information</cite>,
            edited by Edward Barrett. Cambridge, MA:
                The MIT Press, 1989.</div><div class="bibl fallback"><span class="ref" id="bearman2005"><!-- close -->Bearman &amp; Trant 2005</span> Bearman, David, and Jennifer Trant. "Social Terminology Enhancement through Vernacular Engagement:
                Exploring Collaborative Annotation to Encourage Interaction with Museum Collections."<cite class="title italic">D-Lib Magazine</cite>11.9 (2005). Online version at <a class="ref" href="http://www.dlib.org/dlib/september05/bearman/09bearman.html" onclick="window.open('http://www.dlib.org/dlib/september05/bearman/09bearman.html'); return false">http://www.dlib.org/dlib/september05/bearman/09bearman.html</a>.</div><div class="bibl fallback"><span class="ref" id="bra2004"><!-- close -->Bra &amp; Neijdl 2004</span> Bra, Paul de, and Wolfgang Neijdl, eds. <cite class="title italic">Adaptive Hypermedia and Adaptive Web-Based Systems</cite>. Vol.
                3137, <cite class="title italic">Lecture Notes in Computer Science</cite>.
                Berlin: Springer, 2004.
            Online version at <a class="ref" href="http://www.springerlink.com/content/r21696v81c2a/" onclick="window.open('http://www.springerlink.com/content/r21696v81c2a/'); return false">http://www.springerlink.com/content/r21696v81c2a/</a>.</div><div class="bibl fallback"><span class="ref" id="bush1945"><!-- close -->Bush 1945</span> Bush, Vannevar. "As We May Think."<cite class="title italic">Atlantic Monthly</cite>176.1 (1945): 641–649. Online version at <a class="ref" href="http://www.theatlantic.com/doc/194507/bush" onclick="window.open('http://www.theatlantic.com/doc/194507/bush'); return false">http://www.theatlantic.com/doc/194507/bush</a>.</div><div class="bibl fallback"><span class="ref" id="batschmann1986"><!-- close -->Bätschmann 1986</span> Bätschmann, Oskar. <cite class="title italic">Einführung in die
                kunstgeschichtliche Hermeneutik: Die Auslegung von Bildern</cite>.
                Darmstadt: Wissenschaftliche
            Buchgesellschaft, 1986.</div><div class="bibl fallback"><span class="ref" id="chen2005"><!-- close -->Chen &amp; McCleod 2005</span> Chen, Anne Yun-An, and Dennis McCleod. "Collaborative Filtering for Information Recommendation
            Systems." In <cite class="title italic">Encyclopedia of Data Warehousing and
            Mining</cite>. Idea Group, 2005. Online version at
                <a class="ref" href="http://imsc-dmim.usc.edu/publications/121new.pdf" onclick="window.open('http://imsc-dmim.usc.edu/publications/121new.pdf'); return false">http://imsc-dmim.usc.edu/publications/121new.pdf</a>.</div><div class="bibl fallback"><span class="ref" id="eakins1999"><!-- close -->Eakins &amp; Graham 1999</span> Eakins, John P., and Margaret E. Graham. "Content-based Image Retrieval: A report to the JISC Technology
                Applications Programme."Newcastle: Institute for Image Data
            Research, University of Northumbria at Newcastle, 1999.</div><div class="bibl fallback"><span class="ref" id="elkins1999"><!-- close -->Elkins 1999</span> Elkins, James. <cite class="title italic">The Domain of Images</cite>.
                Ithaca and London: Cornell University
            Press, 1999.</div><div class="bibl fallback"><span class="ref" id="finch1974"><!-- close -->Finch 1974</span> Finch, Margaret. <cite class="title italic">Style in Art History</cite>.
                Metuchen, NJ: Scarecrow Press,
                1974.</div><div class="bibl fallback"><span class="ref" id="goodman1988"><!-- close -->Goodman 1988</span> Goodman, Nelson. <cite class="title italic">Languages of Art</cite>.
                Indianapolis: Hackett Publishing
            Company, 1988.</div><div class="bibl fallback"><span class="ref" id="heylighen1999"><!-- close -->Heylighen 1999</span> Heylighen, Francis. "Collaborative
            Filtering." In <cite class="title italic">Principia Cybernetica Web</cite>, <a class="ref" href="http://pespmc1.vub.ac.be/collfilt.html" onclick="window.open('http://pespmc1.vub.ac.be/collfilt.html'); return false">http://pespmc1.vub.ac.be/COLLFILT.html</a>, 1999.</div><div class="bibl fallback"><span class="ref" id="huijsmans1999"><!-- close -->Huijsmans &amp; Smeulders 1999</span> Huijsmans, Dionysius P., and Arnold Smeulders, eds.
                <cite class="title italic">Visual Information and Information Systems: Third International
                Conference, Visual'99, Amsterdam, The Netherlands, June 1999 (Proceedings)</cite>.
                Vol. 1614, <cite class="title italic">Lecture Notes in Computer
            Science</cite>. Berlin: Springer,
                1999.</div><div class="bibl fallback"><span class="ref" id="hutchins1995"><!-- close -->Hutchins 1995</span> Hutchins, Edwin. <cite class="title italic">Cognition in the Wild</cite>.
                Cambridge, MA: The MIT Press,
            1995.</div><div class="bibl fallback"><span class="ref" id="peter2000"><!-- close -->Jenny 2000</span> Peter, Jenny. <cite class="title italic">Bildkonzepte: das wohlgeordnete
                Durcheinander</cite>. Mainz: Verlag Hermann
            Schmidt, 2000.</div><div class="bibl fallback"><span class="ref" id="jorgensen1999"><!-- close -->Jörgensen 1999</span> Jörgensen, Corinne. "Access to Pictorial Material:
                A Review of Current Research and Future Prospects."<cite class="title italic">Computers and the Humanities</cite>33.4 (1999): 293–318. Online version at <a class="ref" href="http://www.springerlink.com/content/w53542u5541x1524/" onclick="window.open('http://www.springerlink.com/content/w53542u5541x1524/'); return false">http://www.springerlink.com/content/w53542u5541x1524/</a>.</div><div class="bibl fallback"><span class="ref" id="kirsch1984"><!-- close -->Kirsch 1984</span> Kirsch, Russell A. "Making Art Historical Sources
                Visible to Computers: Pictures as Primary Sources for Computer-based Art History
                Data." Paper presented at the Automatic Processing of Art History Data and
            Documents, Firenze1984.</div><div class="bibl fallback"><span class="ref" id="loran2006"><!-- close -->Loran 2006</span> Loran, Erle. <cite class="title italic">Cézanne's Composition: Analysis of
                His Form with Diagrams and Photographs of His Motifs</cite>. Berkeley and
                Los Angeles: University of California Press,
                2006.</div><div class="bibl fallback"><span class="ref" id="lynch2001"><!-- close -->Lynch 2001</span> Lynch, Clifford. "Personalization and Recommender
                Systems in the Larger Context: New Directions and Research Questions." In
                <cite class="title italic">Second DELOS Network of Excellence Workshop on Personalisation
                and Recommender Systems in Digital Libraries</cite>. Dublin,
            Ireland, 2001. Online version at <a class="ref" href="http://www.ercim.org/publication/ws-proceedings/delnoe02/cliffordlynchabstract.pdf" onclick="window.open('http://www.ercim.org/publication/ws-proceedings/delnoe02/cliffordlynchabstract.pdf'); return false"> http://www.ercim.org/publication/ws-proceedings/DelNoe02/CliffordLynchAbstract.pdf</a>.</div><div class="bibl fallback"><span class="ref" id="nauta1993"><!-- close -->Nauta 1993</span> Nauta, Gerhard Jan. "HyperIconics: Hypertext and
                the social construction of information about the history of artistic notions."<cite class="title italic">Knowledge Organization</cite>20.1 (1993): 35–46. Online version at <a class="ref" href="http://www.let.leidenuniv.nl/arthis/website/staf/nauta/archive/nautagj_hypericonics_1993_300dpi.pdf" onclick="window.open('http://www.let.leidenuniv.nl/arthis/website/staf/nauta/archive/nautagj_hypericonics_1993_300dpi.pdf'); return false">http://www.let.leidenuniv.nl/arthis/website/staf/nauta/archive/NautaGJ_Hypericonics_1993_300dpi.pdf</a>.</div><div class="bibl fallback"><span class="ref" id="nauta2001"><!-- close -->Nauta 2001</span> Nauta, Gehrard Jan. "Nicolaas Cornelis de
                Gijselaar, Bladzijde uit een prenten-plakboek, midden negentiende eeuw." In
                <cite class="title italic">Uit het Leidse Prentenkabinet</cite>, ed. Nelke
                Bartelings, 135–137. Leiden:
                Primavera Pers, 2001. Online version at <a class="ref" href="http://www.let.leidenuniv.nl/arthis/staf/nauta/archive/nautagj_de_gijselaar_2001_p135-137.pdf" onclick="window.open('http://www.let.leidenuniv.nl/arthis/staf/nauta/archive/nautagj_de_gijselaar_2001_p135-137.pdf'); return false">http://www.let.leidenuniv.nl/arthis/staf/nauta/archive/NautaGJ_de_Gijselaar_2001_p135-137.pdf</a>.</div><div class="bibl fallback"><span class="ref" id="oreilly2006"><!-- close -->O'Reilly 2006</span> O'Reilly, Tim. "Google Image Labeler, the ESP Game,
                and Human-Computer Symbiosis." In <cite class="title italic">O'Reilly Radar
                (weblog)</cite>, <a class="ref" href="http://radar.oreilly.com/archives/2006/09/more_on_google_image_labeler.html" onclick="window.open('http://radar.oreilly.com/archives/2006/09/more_on_google_image_labeler.html'); return false">http://radar.oreilly.com/archives/2006/09/more_on_google_image_labeler.html</a>.
                O'Reilly, 2006.</div><div class="bibl fallback"><span class="ref" id="panofsky1970"><!-- close -->Panofsky 1970</span> Panofsky, Erwin. "Iconography and Iconology: An
                Introduction to the Study of Renaissance Art." In <cite class="title italic">Meaning in the Visual Arts</cite>, 51–81.
            Harmondsworth: Penguin Books,
            1970.</div><div class="bibl fallback"><span class="ref" id="ross2004"><!-- close -->Ross et al. 2004</span> Ross, Seamus, Martin Donnelly, and Milena
                Dobreva. "DigiCULT Technology Watch Report 2 (Core
                Technologies For the Cultural and Scientific Heritage Sector)."Glasgow, 2004. Online version at <a class="ref" href="http://www.digicult.info/downloads/twr_2_2004_final_low.pdf" onclick="window.open('http://www.digicult.info/downloads/twr_2_2004_final_low.pdf'); return false">http://www.digicult.info/downloads/twr_2_2004_final_low.pdf</a>.</div><div class="bibl fallback"><span class="ref" id="salomon1996"><!-- close -->Salomon 1996</span> Salomon, Gavrie. <cite class="title italic">Distributed Cognitions:
                Psychological and Educational Considerations, Learning in Doing: Social, Cognitive
                and Computational Perspectives.</cite>Cambridge: Cambridge University Press,
                1996.</div><div class="bibl fallback"><span class="ref" id="santini2001"><!-- close -->Santini et al. 2001</span> Santini, Simone, Amarnath Gupta, and Ramesh
                Jain. "Emergent Semantics Through Interaction in Image
                Databases."<cite class="title italic">IEEE Transactions on Knowledge and Data Engineering</cite>13.3 (2001): 337–351. Online version at <a class="ref" href="http://www.sdsc.edu/~gupta/publications/kde-sp-01.pdf" onclick="window.open('http://www.sdsc.edu/~gupta/publications/kde-sp-01.pdf'); return false">
                http://www.sdsc.edu/~gupta/publications/kde-sp-01.pdf</a>.</div><div class="bibl fallback"><span class="ref" id="shatfordlayne2002"><!-- close -->Shatford Layne 2002</span> Shatford Layne, Sara. "Subject Access to Art
                Images." In <cite class="title italic">Introduction to Art Image Access: Tools,
                Standards, and Strategies</cite>, ed. Murtha Baca. Los
                Angeles, CA: Getty Research Institute,
            2002. Online version at <a class="ref" href="http://www.getty.edu/research/conducting_research/standards/intro_aia/layne.html" onclick="window.open('http://www.getty.edu/research/conducting_research/standards/intro_aia/layne.html'); return false">http://www.getty.edu/research/conducting_research/standards/intro_aia/layne.html</a>.</div><div class="bibl fallback"><span class="ref" id="sierra2007"><!-- close -->Sierra 2007</span> Sierra, Kathy. "The 'Dumbness of
                Crowds'." In <cite class="title italic">Creating Passionate Users</cite>
            (weblog), <a class="ref" href="http://headrush.typepad.com/creating_passionate_users/2007/01/the_dumbness_of.html" onclick="window.open('http://headrush.typepad.com/creating_passionate_users/2007/01/the_dumbness_of.html'); return false">http://headrush.typepad.com/creating_passionate_users/2007/01/the_dumbness_of.html</a>,
                2007.</div><div class="bibl fallback"><span class="ref" id="stahl2006"><!-- close -->Stahl 2006</span> Stahl, Gerry. <cite class="title italic">Group Cognition: Support for
                Building Collaborative Knowledge</cite>. Cambridge, MA:
                The MIT Press, 2006. Online version at <a class="ref" href="http://www.cis.drexel.edu/faculty/gerry/publications/conferences/2000/icls/index.html" onclick="window.open('http://www.cis.drexel.edu/faculty/gerry/publications/conferences/2000/icls/index.html'); return false">http://www.cis.drexel.edu/faculty/gerry/publications/conferences/2000/icls/index.html</a>.</div><div class="bibl fallback"><span class="ref" id="stubenrauch1993"><!-- close -->Stubenrauch 1993</span> Stubenrauch, Robert, Frank Kappe, and Keith
                Andrews. "Large Hypermedia Systems: The End of the
                Authoring Era." Paper presented at the Educational Multimedia and Hypermedia
            Annual, Orlando, Florida, June 23–26, 1993.</div><div class="bibl fallback"><span class="ref" id="tufte1998"><!-- close -->Tufte 1998</span> Tufte, Edward Rolf. <cite class="title italic">Visual Explanations: Images
                and Quantities, Evidence and Narrative</cite>. Cheshire, CT:
                Graphics Press, 1998.</div><div class="bibl fallback"><span class="ref" id="unsworth2001"><!-- close -->Unsworth 2001</span> Unsworth, John. "Knowledge Representation in
                Humanities Computing." In <cite class="title italic">Lecture I in the eHumanities:
                NEH Lecture Series on Technology and the Humanities</cite>. Washington,
                DC, 2001. Online version at <a class="ref" href="http://www.iath.virginia.edu/~jmu2m/kr/krinhc.html" onclick="window.open('http://www.iath.virginia.edu/~jmu2m/kr/krinhc.html'); return false">http://www.iath.virginia.edu/~jmu2m/KR/KRinHC.html</a>.</div><div class="bibl fallback"><span class="ref" id="vygotsky1976"><!-- close -->Vygotsky 1976</span> Vygotsky, Lev. <cite class="title italic">Mind in Society: The Development of
                Higher Psychological Processes</cite>. Cambridge, MA:
                Harvard University Press, 1976.</div><div class="bibl fallback"><span class="ref" id="wenger2002"><!-- close -->Wenger et al. 2002</span> Wenger, Etienne, Richard McDermott, and
                William M. Snyder. <cite class="title italic">Cultivating Communities of
                Practice</cite>. Boston, MA: Harvard Business
                School Press, 2002.</div><div class="bibl fallback"><span class="ref" id="wyman2006"><!-- close -->Wyman et al. 2006</span> Wyman, Bruce, Susan Chun, Rich
            Cherry, Doug Hiwiller, and Jennifer Trant.
                "Steve.museum: An Ongoing Experiment in Social Tagging,
                Folksonomy, and Museums." In <cite class="title italic">Papers Museums and the Web
                2006</cite>. Pittsburgh: Archives and Museum
                Informatics, 2006. Online version at <a class="ref" href="http://www.archimuse.com/mw2006/papers/wyman/wyman.html" onclick="window.open('http://www.archimuse.com/mw2006/papers/wyman/wyman.html'); return false">http://www.archimuse.com/mw2006/papers/wyman/wyman.html</a>.</div><div class="bibl fallback"><span class="ref" id="wolfflin1950"><!-- close -->Wölfflin 1950</span> Wölfflin, Heinrich. <cite class="title italic">Principles of Art History: The
                Problem of the Development of Style in Later Art</cite>. Unabridged and unaltered
            republication of the Hottinger translation of <cite class="title italic">Kunstgeschichtliche
                Grundbegriffe</cite>, originally published in 1932 by G. Bell and Sons, Ltd. ed.
                Mineola, NY: Dover, 1950.</div></div><div class="toolbar"><a href="../index.html">2008 2.1</a>
             | 
            <a rel="external" href="000019.xml">XML</a>
            | 
            <a href="#" onclick="javascript:window.print();" title="Click for print friendly version">Print Article</a></div></div><script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'dhquarterly'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script><div id="comments"><div id="disqus_thread"/><script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'dhquarterly'; // required: replace example with your forum shortname

    // The following are highly recommended additional parameters. Remove the slashes in front to use.
    var disqus_identifier = '000019';
    var disqus_url = '000019.html';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><div id="footer"> 
            URL: http://www.digitalhumanities.org/dhq/vol/2/1/000019/000019.html<br/>Last updated:
            <script type="text/javascript">
                var monthArray = new initArray("January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December");
                var lastModifiedDate = new Date(document.lastModified);
                var currentDate = new Date();
                document.write(" ",monthArray[(lastModifiedDate.getMonth()+1)]," ");
                document.write(lastModifiedDate.getDate(),", ",(lastModifiedDate.getFullYear()));
            </script><br/> Comments: <a href="mailto:dhqinfo@digitalhumanities.org" class="footer">dhqinfo@digitalhumanities.org</a><br/> Published by:
            <a href="http://www.digitalhumanities.org/" class="footer">The Alliance of Digital Humanities Organizations</a><br/>Affiliated with: <a href="http://llc.oxfordjournals.org/">Literary and Linguistic Computing</a><br/> Copyright 2005 - <script type="text/javascript">
                document.write(currentDate.getFullYear());</script><br/> This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/3.0/">Creative Commons
                    Attribution-Noncommercial-No Derivative Works 3.0 License</a><br/><a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/3.0/"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-nc-nd/3.0/88x31.png"/></a></div></div></div></body>
<!-- Mirrored from www.digitalhumanities.org/dhq/vol/2/1/000019/000019.html by HTTrack Website Copier/3.x [XR&CO'2010], Sat, 16 May 2015 13:18:50 GMT -->
</html>